{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theoretical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What is a Decision Tree, and how does it work  \n",
    "\n",
    "    * A Decision Tree is a supervised machine learning algorithm used for classification and regression. It is a tree-like structure where internal nodes represent features, branches represent decisions, and leaf nodes represent outcomes. The tree is built by recursively splitting the dataset based on the feature that results in the best split according to a chosen criterion\n",
    "    \n",
    "\n",
    "2. What are impurity measures in Decision Trees\t\n",
    "     * Impurity measures quantify the disorder in a dataset. The lower the impurity, the more homogeneous the data. Common impurity measures include:  \n",
    "        i. Gini Impurity: Measures how often a randomly chosen element would be incorrectly classified.   \n",
    "        ii. Entropy: Measures the uncertainty or randomness in the dataset.\n",
    "\n",
    "3. What is the mathematical formula for Gini Impurity\t\n",
    "    * The Gini Impurity for a node is given by:   \n",
    "                Gini=1‚àí ‚àëC pi2   \n",
    "      where ùëùùëñ is the proportion of samples belonging to class i, and C is the number of classes.  \n",
    "\n",
    "4. What is the mathematical formula for Entropy\t\n",
    "\n",
    "Entropy measures the randomness or impurity in a dataset. It is calculated using the formula:\n",
    "\n",
    "\\[\n",
    "Entropy = -\\sum_{i=1}^{C} p_i \\log_2 p_i\n",
    "\\]\n",
    "\n",
    "where:  \n",
    "- \\( C \\) is the total number of classes,  \n",
    "- \\( p_i \\) is the probability of an instance belonging to class \\( i \\).  \n",
    "\n",
    "A lower entropy value indicates a purer node, while a higher entropy value suggests greater disorder.\n",
    "\n",
    "5. What is Information Gain, and how is it used in Decision Trees\t\n",
    "\n",
    "nformation Gain (IG) measures how much a split reduces impurity in a Decision Tree. It is calculated as:\n",
    "\n",
    "\\[\n",
    "IG = Entropy_{parent} - \\sum_{j=1}^{k} \\frac{N_j}{N} Entropy_{child_j}\n",
    "\\]\n",
    "\n",
    "where:  \n",
    "- \\( Entropy_{parent} \\) is the entropy before splitting,  \n",
    "- \\( Entropy_{child_j} \\) is the entropy of child node \\( j \\),  \n",
    "- \\( N_j \\) is the number of samples in child node \\( j \\),  \n",
    "- \\( N \\) is the total number of samples in the parent node,  \n",
    "- \\( k \\) is the number of child nodes.  \n",
    "\n",
    "A higher Information Gain means a feature provides a better split, leading to a more effective Decision Tree. The feature with the highest Information Gain is chosen at each step during tree construction.\n",
    "\n",
    "6. What is the difference between Gini Impurity and Entropy\t\n",
    "\n",
    "    * Gini Impurity is computationally simpler and tends to favor larger class splits.\n",
    "    * Entropy involves logarithms, making it more computationally expensive but often leading to better decision boundaries.\n",
    "\n",
    "7. What is the mathematical explanation behind Decision Trees\t\n",
    "    * Select the best feature to split using an impurity measure (e.g., Gini or Entropy).\n",
    "    * Split the dataset at the chosen feature value.\n",
    "    * Recursively apply the process to child nodes until a stopping criterion is met (e.g., minimum samples per leaf).\n",
    "    * Assign the most frequent class (classification) or mean value (regression) to leaf nodes.\n",
    "\n",
    "8. What is Pre-Pruning in Decision Trees\n",
    "    * Pre-pruning stops the tree from growing beyond a certain depth or condition (e.g., minimum samples per split). This help prevent overfitting by limiting complexity.\n",
    "\n",
    "9. What is the process of Post-Pruning in Decision Trees\n",
    "\n",
    "Post-pruning is a technique used to reduce overfitting in Decision Trees by trimming unnecessary branches after the tree has been fully grown. The goal is to improve generalization by simplifying the model.\n",
    "\n",
    "1. Grow the Full Tree \n",
    "   - The Decision Tree is trained to its maximum depth, capturing all possible patterns in the data.\n",
    "\n",
    "2. Evaluate Performance on Validation Data\n",
    "   - A separate validation set (or cross-validation) is used to assess the performance of the tree.\n",
    "\n",
    "3. Prune the Least Important Nodes\n",
    "   - Nodes or branches that do not significantly improve the model's performance are removed.  \n",
    "   - This is done by comparing the accuracy before and after pruning.  \n",
    "   - Two common pruning strategies:\n",
    "     - Cost Complexity Pruning (CCP): Introduces a penalty for tree complexity.\n",
    "     - Reduced Error Pruning: Directly removes nodes and checks if accuracy improves.\n",
    "\n",
    "4. Stop When Performance Decreases  \n",
    "   - Pruning continues until removing further branches reduces validation accuracy.   \n",
    "\n",
    "    By eliminating redundant splits, post-pruning leads to a more interpretable and efficient Decision Tree that generalizes better to unseen data.\n",
    "\n",
    "\n",
    "10. What is the difference between Pre-Pruning and Post-Pruning\t\n",
    "\n",
    "    * Pre-Pruning: Stops the tree from growing too deep based on predefined constraints.\n",
    "    * Post-Pruning: Allows the tree to grow fully and then removes branches that do not contribute significantly to accuracy.\n",
    "\n",
    "11. What is a Decision Tree Regressor\t\n",
    "\n",
    "    * A Decision Tree Regressor is a type of Decision Tree used for regression tasks. Instead of predicting categorical classes, it predicts continuous values by splitting the dataset based on minimizing variance at each node.\n",
    "\n",
    "12. What are the advantages and disadvantages of Decision Trees\t\n",
    "    * Advantages:\n",
    "    \n",
    "        * Easy to interpret and visualize.\n",
    "        * Handles both categorical and numerical data.\n",
    "        * No need for feature scaling.\n",
    "        * Works well with small to medium-sized datasets.\n",
    "\n",
    "    * Disadvantages:\n",
    "\n",
    "        * Prone to overfitting.\n",
    "        * Unstable (small changes in data can lead to different trees).\n",
    "        * Greedy splitting can lead to suboptimal trees.\n",
    "\n",
    "14. How does a Decision Tree handle categorical features\t\n",
    "\n",
    "    * Categorical features are usually converted into numerical values using one-hot encoding or label encoding.\n",
    "    * The tree can also split directly on categorical values by grouping categories together during the split.\n",
    "\n",
    "15. What are some real-world applications of Decision Trees?\n",
    "\n",
    "    * Healthcare: Diagnosing diseases based on symptoms.\n",
    "    * Finance: Credit risk assessment.\n",
    "    * E-commerce: Customer segmentation and recommendation systems.\n",
    "    * Manufacturing: Quality control and defect detection.\n",
    "    * Marketing: Predicting customer churn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import graphviz\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier , DecisionTreeRegressor , export_graphviz\n",
    "from sklearn.metrics import accuracy_score , mean_squared_error\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data 0f iris\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16. Write a Python program to train a Decision Tree Classifier on the Iris dataset and print the model accuracy\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 17. Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the feature importances\n",
    "\n",
    "feature_names = iris.feature_names\n",
    "\n",
    "clf = DecisionTreeClassifier(criterion=\"gini\", random_state=1)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Feature Importances:\")\n",
    "for feature, importance in zip(feature_names, clf.feature_importances_):\n",
    "    print(f\"{feature}: {importance:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18. Write a Python program to train a Decision Tree Classifier using Entropy as the splitting criterion and print the model accuracy\n",
    "\n",
    "clf = DecisionTreeClassifier(criterion=\"entropy\", random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 19. Write a Python program to train a Decision Tree Regressor on a housing dataset and evaluate using Mean Squared Error (MSE)\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X, y = housing.data, housing.target\n",
    "\n",
    "# Split the dataset into training (80%) and testing (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Train the Decision Tree Regressor\n",
    "regressor = DecisionTreeRegressor(random_state=42)\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "# Compute and print Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20. Write a Python program to train a Decision Tree Classifier and visualize the tree using graphviz\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "clf = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=1)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Model Accuracy: {accuracy:.2f}')\n",
    "\n",
    "feature_names = iris.feature_names\n",
    "class_names = iris.target_names\n",
    "dot_data = export_graphviz(\n",
    "    clf, out_file=None, feature_names=feature_names, class_names=class_names,\n",
    "    filled=True, rounded=True, special_characters=True\n",
    ")\n",
    "\n",
    "graph = graphviz.Source(dot_data)\n",
    "graph.render(\"decision_tree\") \n",
    "\n",
    "graph.view()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 21. Write a Python program to train a Decision Tree Classifier with a maximum depth of 3 and compare its accuracy with a fully grown tree\n",
    "\n",
    "clf_limited = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=1)\n",
    "clf_limited.fit(X_train, y_train)\n",
    "\n",
    "y_pred_limited = clf_limited.predict(X_test)\n",
    "accuracy_limited = accuracy_score(y_test, y_pred_limited)\n",
    "print(f'Model Accuracy (max_depth=3): {accuracy_limited:.2f}')\n",
    "\n",
    "clf_full = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
    "clf_full.fit(X_train, y_train)\n",
    "\n",
    "y_pred_full = clf_full.predict(X_test)\n",
    "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
    "print(f'Model Accuracy (fully grown): {accuracy_full:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 22. Write a Python program to train a Decision Tree Classifier using min_samples_split=5 and compare its accuracy with a default tree\n",
    "\n",
    "clf_min_samples = DecisionTreeClassifier(criterion='gini', min_samples_split=5, random_state=42)\n",
    "clf_min_samples.fit(X_train, y_train)\n",
    "\n",
    "y_pred_min_samples = clf_min_samples.predict(X_test)\n",
    "accuracy_min_samples = accuracy_score(y_test, y_pred_min_samples)\n",
    "print(f'Model Accuracy (min_samples_split=5): {accuracy_min_samples:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 23. Write a Python program to apply feature scaling before training a Decision Tree Classifier and compare its accuracy with unscaled data\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "clf_scaled = DecisionTreeClassifier(criterion='gini', random_state=1)\n",
    "clf_scaled.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred_scaled = clf_scaled.predict(X_test_scaled)\n",
    "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
    "print(f'Model Accuracy (with feature scaling): {accuracy_scaled:.2f}')\n",
    "\n",
    "feature_names = iris.feature_names\n",
    "class_names = iris.target_names\n",
    "dot_data = export_graphviz(\n",
    "    clf_limited, out_file=None, feature_names=feature_names, class_names=class_names,\n",
    "    filled=True, rounded=True, special_characters=True\n",
    ")\n",
    "\n",
    "graph = graphviz.Source(dot_data)\n",
    "graph.render(\"decision_tree\")  \n",
    "\n",
    "graph.view()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 24. Write a Python program to train a Decision Tree Classifier using One-vs-Rest (OvR) strategy for multiclass classification\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "ovr_classifier = OneVsRestClassifier(DecisionTreeClassifier(criterion='gini', random_state=1))\n",
    "ovr_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = ovr_classifier.predict(X_test)\n",
    "\n",
    "# Calculate and print accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Model Accuracy (One-vs-Rest Decision Tree): {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 25. Write a Python program to train a Decision Tree Classifier and display the feature importance scores\n",
    "\n",
    "clf = DecisionTreeClassifier(criterion='gini', random_state=1)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importance scores\n",
    "feature_importances = clf.feature_importances_\n",
    "\n",
    "# Display feature importance scores\n",
    "print(\"Feature Importance Scores:\")\n",
    "for feature, importance in zip(feature_names, feature_importances):\n",
    "    print(f\"{feature}: {importance:.4f}\")\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.barh(feature_names, feature_importances, color='skyblue')\n",
    "plt.xlabel(\"Feature Importance Score\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.title(\"Feature Importance in Decision Tree Classifier\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 26. Write a Python program to train a Decision Tree Regressor with max_depth=5 and compare its performance with an unrestricted tree\n",
    "\n",
    "X, y = housing.data, housing.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "regressor_limited = DecisionTreeRegressor(max_depth=5, random_state=42)\n",
    "regressor_limited.fit(X_train, y_train)\n",
    "\n",
    "regressor_full = DecisionTreeRegressor(random_state=42)\n",
    "regressor_full.fit(X_train, y_train)\n",
    "\n",
    "y_pred_limited = regressor_limited.predict(X_test)\n",
    "y_pred_full = regressor_full.predict(X_test)\n",
    "\n",
    "mse_limited = mean_squared_error(y_test, y_pred_limited)\n",
    "mse_full = mean_squared_error(y_test, y_pred_full)\n",
    "\n",
    "# Display results\n",
    "print(f'Mean Squared Error (max_depth=5): {mse_limited:.4f}')\n",
    "print(f'Mean Squared Error (fully grown tree): {mse_full:.4f}')\n",
    "\n",
    "# Plot actual vs predicted values\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(y_test, y_pred_limited, color='blue', label='max_depth=5', alpha=0.6)\n",
    "plt.scatter(y_test, y_pred_full, color='red', label='Fully grown tree', alpha=0.4)\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], linestyle=\"--\", color=\"black\")\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(\"Decision Tree Regression: Actual vs Predicted\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 27. Write a Python program to train a Decision Tree Classifier, apply Cost Complexity Pruning (CCP), and visualize its effect on accuracy\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Train a fully grown Decision Tree to determine ccp_alphas\n",
    "clf_full = DecisionTreeClassifier(random_state=42)\n",
    "clf_full.fit(X_train, y_train)\n",
    "\n",
    "# Get the Cost Complexity Pruning Path\n",
    "path = clf_full.cost_complexity_pruning_path(X_train, y_train)\n",
    "ccp_alphas = path.ccp_alphas[:-1]  # Exclude the maximum value (prunes all leaves)\n",
    "\n",
    "# Train Decision Trees using different ccp_alpha values\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "for alpha in ccp_alphas:\n",
    "    clf = DecisionTreeClassifier(random_state=42, ccp_alpha=alpha)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Compute accuracies\n",
    "    train_acc = accuracy_score(y_train, clf.predict(X_train))\n",
    "    test_acc = accuracy_score(y_test, clf.predict(X_test))\n",
    "\n",
    "    train_accuracies.append(train_acc)\n",
    "    test_accuracies.append(test_acc)\n",
    "\n",
    "# Plot Accuracy vs CCP Alpha\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(ccp_alphas, train_accuracies, marker='o', label=\"Train Accuracy\", color='blue')\n",
    "plt.plot(ccp_alphas, test_accuracies, marker='o', label=\"Test Accuracy\", color='red')\n",
    "plt.xlabel(\"CCP Alpha\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Effect of Cost Complexity Pruning on Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 28. Write a Python program to train a Decision Tree Classifier and evaluate its performance using Precision, Recall, and F1-Score\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the Decision Tree Classifier\n",
    "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model using Precision, Recall, and F1-Score\n",
    "report = classification_report(y_test, y_pred, target_names=iris.target_names)\n",
    "print(\"Classification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 29. Write a Python program to train a Decision Tree Classifier and visualize the confusion matrix using seaborn\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the Decision Tree Classifier\n",
    "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=iris.target_names, yticklabels=iris.target_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 30. Write a Python program to train a Decision Tree Classifier and use GridSearchCV to find the optimal values for max_depth and min_samples_split.\n",
    "\n",
    "\n",
    "from sklearn.model_selection import  GridSearchCV\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 10, None],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model and parameters\n",
    "best_clf = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "\n",
    "# Evaluate on the test set\n",
    "y_pred = best_clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Set Accuracy: {accuracy:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
