{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theoretical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What is a Support Vector Machine (SVM)?\n",
    "\n",
    "* SVM is a supervised learning algorithm used for classification and regression tasks. It finds the optimal hyperplane that best separates different classes in a dataset.\n",
    "\n",
    "2. What is the difference between Hard Margin and Soft Margin SVM\n",
    "\n",
    "* Hard Margin SVM: Requires all points to be correctly classified with no tolerance for misclassification (only works for linearly separable data).\n",
    "* Soft Margin SVM: Allows some misclassification by introducing a slack variable, making it more robust for non-linearly separable data.\n",
    "\n",
    "3. What is the mathematical intuition behind SVM\n",
    "\n",
    "* SVM maximizes the margin (distance) between the decision boundary (hyperplane) and the closest data points (support vectors).\n",
    "* It solves the optimization problem max x 1 / ||w||   Subject to constraints ensuring correct classification: ùë¶ùëñ(ùë§‚ãÖùë•ùëñ+ùëè)‚â•1yi(w‚ãÖx i+b)‚â•1\n",
    "\n",
    "4. What is the role of Lagrange Multipliers in SVM  \n",
    "* Used in constrained optimization to transform the problem into a Lagrangian formulation, enabling the use of kernel tricks.\n",
    "* Converts the problem into a dual form, making computations more efficient.\n",
    "\n",
    "5. What are Support Vectors in SVM\n",
    "\n",
    "* They are the critical data points that lie closest to the decision boundary and influence the hyperplane's position.\n",
    "\n",
    "6. What are Support Vector Classifier (SVC)\n",
    "\n",
    "* A classification variant of SVM that determines the best decision boundary to separate classes.\n",
    "\n",
    "7. What are Support Vector Regressor (SVR):\n",
    "\n",
    "* A regression variant that finds a hyperplane that fits the data while minimizing the error within a certain margin.\n",
    "\n",
    "8. What is the Kernel Trick in SVM?\n",
    "\n",
    "* Allows SVM to work in a higher-dimensional space without explicitly computing transformations, making it effective for non-linearly separable data\n",
    "\n",
    "9.  Compare Linear Kernel, Polynomial Kernel, and RBF Kernel\n",
    "\n",
    "* Linear Kernel: K(x,y)=x‚ãÖy (for linearly separable data).\n",
    "* Polynomial Kernel: K(x,y)=(x‚ãÖy+c)**d(captures complex relationships).\n",
    "* Radial Basis Function (RBF) Kernel: K(x,y)=e ‚àíŒ≥‚à•x‚àíy‚à• 2(effective for highly non-linear data).\n",
    "\n",
    "10. What is the Effect of the C Parameter in SVM\n",
    "\n",
    "* Controls the trade-off between maximizing margin and minimizing misclassification.\n",
    "* High C ‚Üí Less margin, more importance on correct classification.\n",
    "* Low C ‚Üí Larger margin, allows more misclassification.\n",
    "\n",
    "11.  What is the Role of Gamma Parameter in RBF Kernel SVM:\n",
    "\n",
    "* Defines how far influence of a single training example extends.\n",
    "* High gamma ‚Üí More complex model (may overfit).\n",
    "* Low gamma ‚Üí Simpler model (may underfit).\n",
    "\n",
    "12. What is the Na√Øve Bayes Classifier, and Why is it Called \"Na√Øve\"\n",
    "\n",
    "* A probabilistic classifier based on Bayes' Theorem with the assumption that features are conditionally independent given the class.\n",
    "* \"Na√Øve\" because this independence assumption is often unrealistic in real-world data.\n",
    "\n",
    "13. What is Bayes‚Äô Theorem:\n",
    "\n",
    "* P(A‚à£B)= P(B)P(A)\\P(B) Used to calculate the probability of a class given observed features.\n",
    "\n",
    "14. : Explain the differences between Gaussian Na√Øve Bayes, Multinomial Na√Øve Bayes, and Bernoulli Na√Øve Bayes:\n",
    "* Gaussian Na√Øve Bayes: Assumes continuous features follow a Gaussian (Normal) distribution.\n",
    "* Multinomial Na√Øve Bayes: Used for text classification (word frequencies).\n",
    "* Bernoulli Na√Øve Bayes: Works with binary features (e.g., word presence/absence in text).\n",
    "\n",
    "15. When should you use Gaussian Na√Øve Bayes over other variants\n",
    "\n",
    "* When features are continuous and normally distributed (e.g., height, weight, age).\n",
    "\n",
    "16. What are the key assumptions made by Na√Øve Bayes\n",
    "* Feature independence.\n",
    "* Equal importance of all features.\n",
    "* Conditional probability distributions remain stable.\n",
    "\n",
    "17. What are the advantages and disadvantages of Na√Øve Bayes\n",
    "* Advantages:\n",
    "    * Fast training and inference.\n",
    "    * Works well with small datasets.\n",
    "    * Good for high-dimensional data (text classification).\n",
    "* Disadvantages:\n",
    "    * Independence assumption rarely holds in real data.\n",
    "    * Poor performance when features are strongly correlated.\n",
    "\n",
    "18. Why is Na√Øve Bayes a good choice for text classification\n",
    "* Works well with word frequencies and bag-of-words models.\n",
    "* Efficient with high-dimensional sparse data.\n",
    "* Assumption of independence is reasonable for many text applications.\n",
    "\n",
    "19. : Compare SVM and Na√Øve Bayes for classification tasks\n",
    "* SVM: Better for complex decision boundaries and continuous data.\n",
    "* Na√Øve Bayes: Better for text classification and probabilistic interpretations.\n",
    "\n",
    "20. How does Laplace Smoothing help in Na√Øve Bayes?\n",
    "* Handles zero probabilities when a feature-class combination is missing in training.\n",
    "* Adds a small constant (Œ±) to all feature counts:\n",
    "\n",
    "               P(x‚à£y)= count(y)+Œ±‚ãÖN/count(x,y)+Œ±\n",
    "‚Äã\n",
    " Prevents division by zero and improves generalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import accuracy_score , mean_squared_error ,  auc\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 21. Write a Python program to train an SVM Classifier on the Iris dataset and evaluate accuracy\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "model = SVC(kernel='linear')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"SVM Classifier Accuracy on Iris dataset: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 22. Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies:\n",
    "\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "wine = load_wine()\n",
    "X, y = wine.data, wine.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "svm_linear = SVC(kernel='linear')\n",
    "svm_rbf = SVC(kernel='rbf')\n",
    "\n",
    "svm_linear.fit(X_train, y_train)\n",
    "svm_rbf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred_linear = svm_linear.predict(X_test)\n",
    "y_pred_rbf = svm_rbf.predict(X_test)\n",
    "\n",
    "acc_linear = accuracy_score(y_test, y_pred_linear)\n",
    "acc_rbf = accuracy_score(y_test, y_pred_rbf)\n",
    "\n",
    "print(f\"Linear Kernel Accuracy: {acc_linear:.4f}\")\n",
    "print(f\"RBF Kernel Accuracy: {acc_rbf:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 23. Write a Python program to train an SVM Regressor (SVR) on a housing dataset and evaluate it using Mean Squared Error (MSE):\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# Load the California housing dataset\n",
    "housing = fetch_california_housing()\n",
    "X, y = housing.data, housing.target\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train an SVM Regressor\n",
    "svr = SVR(kernel='rbf')\n",
    "svr.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = svr.predict(X_test)\n",
    "\n",
    "# Evaluate with Mean Squared Error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"SVR Mean Squared Error on Housing dataset: {mse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 24. Write a Python program to train an SVM Classifier with a Polynomial Kernel and visualize the decision boundary\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=100, noise=0.1, random_state=42)\n",
    "\n",
    "# Train an SVM classifier with a Polynomial Kernel\n",
    "svm_poly = SVC(kernel='poly', degree=3)\n",
    "svm_poly.fit(X, y)\n",
    "\n",
    "# Plot decision boundary\n",
    "def plot_decision_boundary(model, X, y):\n",
    "    x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1\n",
    "    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                         np.linspace(y_min, y_max, 100))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    plt.contourf(xx, yy, Z, alpha=0.3)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k')\n",
    "    plt.title(\"SVM with Polynomial Kernel\")\n",
    "    plt.show()\n",
    "\n",
    "plot_decision_boundary(svm_poly, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 25.  Write a Python program to train a Gaussian Na√Øve Bayes classifier on the Breast Cancer dataset and evaluate accuracy\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "X, y = cancer.data, cancer.target\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Gaussian Na√Øve Bayes classifier\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = gnb.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Gaussian Na√Øve Bayes Accuracy on Breast Cancer dataset: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 26.  Write a Python program to train a Multinomial Na√Øve Bayes classifier for text classification using the 20  Newsgroups dataset.\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "categories = ['rec.sport.baseball', 'sci.space', 'comp.graphics', 'talk.politics.mideast']\n",
    "newsgroups = fetch_20newsgroups(subset='all', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(newsgroups.data, newsgroups.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a text classification pipeline\n",
    "model = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Multinomial Na√Øve Bayes Accuracy on 20 Newsgroups dataset: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 27. Write a Python program to train an SVM Classifier with different C values and compare the decision boundaries visually\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=200, noise=0.2, random_state=42)\n",
    "\n",
    "C_values = [0.1, 1, 10, 100]\n",
    "\n",
    "def plot_decision_boundary(model, X, y, ax, title):\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                         np.linspace(y_min, y_max, 100))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    ax.contourf(xx, yy, Z, alpha=0.3)\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k')\n",
    "    ax.set_title(title)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 8))\n",
    "\n",
    "for i, C in enumerate(C_values):\n",
    "    model = SVC(kernel='linear', C=C)\n",
    "    model.fit(X, y)\n",
    "    plot_decision_boundary(model, X, y, axes[i // 2, i % 2], f\"C = {C}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 28. Write a Python program to train a Bernoulli Na√Øve Bayes classifier for binary classification on a dataset with binary features\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "np.random.seed(42)\n",
    "X = np.random.randint(2, size=(1000, 10))  # Binary features (0 or 1)\n",
    "y = np.random.randint(2, size=(1000,))  # Binary labels (0 or 1)\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Bernoulli Na√Øve Bayes model\n",
    "bnb = BernoulliNB()\n",
    "bnb.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = bnb.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Bernoulli Na√Øve Bayes Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 29.  Write a Python program to apply feature scaling before training an SVM model and compare results with unscaled data\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train SVM without scaling\n",
    "svm_unscaled = SVC(kernel='rbf')\n",
    "svm_unscaled.fit(X_train, y_train)\n",
    "y_pred_unscaled = svm_unscaled.predict(X_test)\n",
    "accuracy_unscaled = accuracy_score(y_test, y_pred_unscaled)\n",
    "\n",
    "# Apply feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train SVM with scaled data\n",
    "svm_scaled = SVC(kernel='rbf')\n",
    "svm_scaled.fit(X_train_scaled, y_train)\n",
    "y_pred_scaled = svm_scaled.predict(X_test_scaled)\n",
    "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
    "\n",
    "print(f\"SVM Accuracy without Scaling: {accuracy_unscaled:.4f}\")\n",
    "print(f\"SVM Accuracy with Scaling: {accuracy_scaled:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 30.  Write a Python program to train a Gaussian Na√Øve Bayes model and compare the predictions before and after Laplace Smoothing\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "X, y = cancer.data, cancer.target\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "gnb = GaussianNB(var_smoothing=1e-9) \n",
    "gnb.fit(X_train, y_train)\n",
    "y_pred_no_smoothing = gnb.predict(X_test)\n",
    "accuracy_no_smoothing = accuracy_score(y_test, y_pred_no_smoothing)\n",
    "\n",
    "\n",
    "gnb_smooth = GaussianNB(var_smoothing=1e-2)  \n",
    "gnb_smooth.fit(X_train, y_train)\n",
    "y_pred_smooth = gnb_smooth.predict(X_test)\n",
    "accuracy_smooth = accuracy_score(y_test, y_pred_smooth)\n",
    "\n",
    "print(f\"Gaussian NB Accuracy without Smoothing: {accuracy_no_smoothing:.4f}\")\n",
    "print(f\"Gaussian NB Accuracy with Smoothing: {accuracy_smooth:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 31. Write a Python program to train an SVM Classifier and use GridSearchCV to tune the hyperparameters (C, gamma, kernel)\n",
    "\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "wine = load_wine()\n",
    "X, y = wine.data, wine.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'gamma': ['scale', 0.1, 1],\n",
    "    'kernel': ['linear', 'rbf']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(SVC(), param_grid, cv=5, scoring='accuracy', verbose=1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best accuracy: {grid_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 32. Write a Python program to train an SVM Classifier on an imbalanced dataset and apply class weighting and check it improve accuracy\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_features=20, weights=[0.9, 0.1], random_state=42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "svm_unweighted = SVC(kernel='rbf', class_weight=None)\n",
    "svm_unweighted.fit(X_train, y_train)\n",
    "y_pred_unweighted = svm_unweighted.predict(X_test)\n",
    "accuracy_unweighted = accuracy_score(y_test, y_pred_unweighted)\n",
    "\n",
    "svm_weighted = SVC(kernel='rbf', class_weight='balanced')\n",
    "svm_weighted.fit(X_train, y_train)\n",
    "y_pred_weighted = svm_weighted.predict(X_test)\n",
    "accuracy_weighted = accuracy_score(y_test, y_pred_weighted)\n",
    "\n",
    "print(f\"SVM Accuracy without Class Weighting: {accuracy_unweighted:.4f}\")\n",
    "print(f\"SVM Accuracy with Class Weighting: {accuracy_weighted:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 33. Write a Python program to implement a Na√Øve Bayes classifier for spam detection using email data\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/justmarkham/pycon-2016-tutorial/master/data/sms-spam-collection/spam.csv', encoding='latin-1')[['v1', 'v2']]\n",
    "df.columns = ['label', 'message']\n",
    "\n",
    "\n",
    "df['label'] = df['label'].map({'ham': 0, 'spam': 1})\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['message'], df['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "spam_clf = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "\n",
    "spam_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = spam_clf.predict(X_test)\n",
    "print(f\"Spam Detector Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 34. Write a Python program to train an SVM Classifier and a Na√Øve Bayes Classifier on the same dataset and compare their accuracy\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()\n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.2, random_state=42)\n",
    "\n",
    "svm_clf = SVC(kernel='rbf', C=1.0)\n",
    "svm_clf.fit(X_train, y_train)\n",
    "y_pred_svm = svm_clf.predict(X_test)\n",
    "\n",
    "nb_clf = GaussianNB()\n",
    "nb_clf.fit(X_train, y_train)\n",
    "y_pred_nb = nb_clf.predict(X_test)\n",
    "\n",
    "print(f\"SVM Accuracy: {accuracy_score(y_test, y_pred_svm):.4f}\")\n",
    "print(f\"Na√Øve Bayes Accuracy: {accuracy_score(y_test, y_pred_nb):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 35. Write a Python program to perform feature selection before training a Na√Øve Bayes classifier and compare results\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "feature_selector = SelectKBest(chi2, k=20)\n",
    "\n",
    "nb_pipeline = make_pipeline(feature_selector, GaussianNB())\n",
    "nb_pipeline.fit(X_train, y_train)\n",
    "y_pred_nb_fs = nb_pipeline.predict(X_test)\n",
    "\n",
    "print(f\"Na√Øve Bayes Accuracy with Feature Selection: {accuracy_score(y_test, y_pred_nb_fs):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 36. Write a Python program to train an SVM Classifier using One-vs-Rest (OvR) and One-vs-One (OvO) strategies on the Wine dataset and compare their accuracy=\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
    "\n",
    "wine = load_wine()\n",
    "X_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "svm_ovr = OneVsRestClassifier(SVC(kernel='rbf', C=1.0))\n",
    "svm_ovr.fit(X_train, y_train)\n",
    "y_pred_ovr = svm_ovr.predict(X_test)\n",
    "\n",
    "svm_ovo = OneVsOneClassifier(SVC(kernel='rbf', C=1.0))\n",
    "svm_ovo.fit(X_train, y_train)\n",
    "y_pred_ovo = svm_ovo.predict(X_test)\n",
    "\n",
    "\n",
    "print(f\"OvR SVM Accuracy: {accuracy_score(y_test, y_pred_ovr):.4f}\")\n",
    "print(f\"OvO SVM Accuracy: {accuracy_score(y_test, y_pred_ovo):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 37. Write a Python program to train an SVM Classifier using Linear, Polynomial, and RBF kernels on the Breast Cancer dataset and compare their accuracy=\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, test_size=0.2, random_state=42)\n",
    "\n",
    "kernels = ['linear', 'poly', 'rbf']\n",
    "for kernel in kernels:\n",
    "    svm = SVC(kernel=kernel, C=1.0)\n",
    "    svm.fit(X_train, y_train)\n",
    "    y_pred = svm.predict(X_test)\n",
    "    print(f\"SVM Accuracy with {kernel} kernel: {accuracy_score(y_test, y_pred):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 38. Write a Python program to train an SVM Classifier using Stratified K-Fold Cross-Validation and compute the average accuracy\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "svm = SVC(kernel='rbf', C=1.0)\n",
    "\n",
    "scores = cross_val_score(svm, cancer.data, cancer.target, cv=skf, scoring='accuracy')\n",
    "print(f\"Stratified K-Fold SVM Average Accuracy: {scores.mean():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 39. Write a Python program to train a Na√Øve Bayes classifier using different prior probabilities and compare performance\n",
    "\n",
    "\n",
    "nb_default = GaussianNB()\n",
    "nb_default.fit(X_train, y_train)\n",
    "y_pred_default = nb_default.predict(X_test)\n",
    "\n",
    "custom_priors = [0.7, 0.3]  \n",
    "nb_custom = GaussianNB(priors=custom_priors)\n",
    "nb_custom.fit(X_train, y_train)\n",
    "y_pred_custom = nb_custom.predict(X_test)\n",
    "\n",
    "\n",
    "print(f\"Na√Øve Bayes Accuracy with Default Priors: {accuracy_score(y_test, y_pred_default):.4f}\")\n",
    "print(f\"Na√Øve Bayes Accuracy with Custom Priors: {accuracy_score(y_test, y_pred_custom):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 40. Write a Python program to perform Recursive Feature Elimination (RFE) before training an SVM Classifier and compare accuracy\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "wine = load_wine()\n",
    "X_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "svm = SVC(kernel='linear', C=1.0)\n",
    "rfe = RFE(svm, n_features_to_select=5)\n",
    "X_train_rfe = rfe.fit_transform(X_train, y_train)\n",
    "X_test_rfe = rfe.transform(X_test)\n",
    "\n",
    "\n",
    "svm.fit(X_train_rfe, y_train)\n",
    "y_pred = svm.predict(X_test_rfe)\n",
    "\n",
    "print(f\"SVM Accuracy after RFE: {accuracy_score(y_test, y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 41. Write a Python program to train an SVM Classifier and evaluate its performance using Precision, Recall, and F1-Score instead of accuracy\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "svm = SVC(kernel='rbf', C=1.0)\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 42. Write a Python program to train a Na√Øve Bayes Classifier and evaluate its performance using Log Loss (Cross-Entropy Loss)\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train, y_train)\n",
    "y_prob = nb.predict_proba(X_test)\n",
    "\n",
    "log_loss_value = log_loss(y_test, y_prob)\n",
    "print(f\"Na√Øve Bayes Log Loss: {log_loss_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 43. Write a Python program to train an SVM Classifier and visualize the Confusion Matrix using seaborn\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=wine.target_names, yticklabels=wine.target_names)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"SVM Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 44. Write a Python program to train an SVM Regressor (SVR) and evaluate its performance using Mean Absolute Error (MAE) instead of MSE\n",
    "\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "diabetes = load_diabetes()\n",
    "X_train, X_test, y_train, y_test = train_test_split(diabetes.data, diabetes.target, test_size=0.2, random_state=42)\n",
    "\n",
    "svr = SVR(kernel='rbf', C=1.0)\n",
    "svr.fit(X_train, y_train)\n",
    "y_pred = svr.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f\"SVR Mean Absolute Error: {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 45. Write a Python program to train a Na√Øve Bayes classifier and evaluate its performance using the ROC-AUC score\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target  \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_probs = model.predict_proba(X_test)[:, 1]  \n",
    "\n",
    "\n",
    "roc_auc = roc_auc_score(y_test, y_probs)\n",
    "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, y_probs)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='blue', label=f'ROC Curve (AUC = {roc_auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='red', linestyle='--')  # Diagonal line\n",
    "plt.xlabel(\"False Positive Rate (FPR)\")\n",
    "plt.ylabel(\"True Positive Rate (TPR)\")\n",
    "plt.title(\"ROC Curve for Na√Øve Bayes Classifier\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 46 Write a Python program to train an SVM Classifier and visualize the Precision-Recall Curve\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target  \n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "model = SVC(kernel='rbf', probability=True, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_probs = model.predict_proba(X_test)[:, 1]  \n",
    "\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_probs)\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall, precision, color='blue', label=f'Precision-Recall Curve (AUC = {pr_auc:.4f})')\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve for SVM Classifier\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
