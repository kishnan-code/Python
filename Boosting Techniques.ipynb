{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theoretical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What is Boosting in Machine Learning  \n",
    "    * Boosting is an ensemble learning technique that combines multiple weak learners (typically decision trees) to create a strong predictive model. It sequentially trains models, giving more weight to misclassified instances, thereby improving overall performance.\n",
    "\n",
    "2. How does Boosting differ from Bagging\n",
    "    * Boosting focuses on sequentially improving weak learners by adjusting weights for misclassified samples.\n",
    "    * Bagging (e.g., Random Forest) trains models independently in parallel and averages predictions to reduce var\n",
    "\n",
    "\n",
    "3. What is the key idea behind AdaBoost\n",
    "    * The key idea of AdaBoost (Adaptive Boosting) is to assign higher weights to misclassified samples in each iteration, so subsequent models focus more on difficult examples, thereby improving accuracy.\n",
    "\n",
    "\n",
    "4. Explain the working of AdaBoost with an example\n",
    "    * Assign equal weights to all training samples.\n",
    "    * Train a weak classifier (e.g., decision stump).\n",
    "    * Increase the weights of misclassified samples.\n",
    "    * Train the next classifier with updated weights.\n",
    "    * Repeat for multiple iterations and combine classifiers using weighted voting.\n",
    "    * Example: If a weak classifier misclassifies a few points in a dataset, those points get more importance in the next iteration, so the model focuses on them.\n",
    "\n",
    "5. What is Gradient Boosting, and how is it different from AdaBoost\n",
    "    * Gradient Boosting minimizes a loss function by iteratively adding new weak models that correct the residual errors of previous models. Unlike AdaBoost, which adjusts weights, Gradient Boosting builds trees based on the gradient of the loss function.\n",
    "\n",
    "6. What is the loss function in Gradient Boosting\n",
    "    * The loss function depends on the task:\n",
    "        - Regression: Mean Squared Error (MSE)\n",
    "        - Classification: Log Loss (Cross-Entropy)\n",
    "        - Custom Loss: Can be any differentiable function.\n",
    "\n",
    "7. How does XGBoost improve over traditional Gradient Boosting\n",
    "    * Using regularization (L1/L2) to reduce overfitting.\n",
    "    * Tree pruning instead of greedy splitting.\n",
    "    * Efficient parallel processing.\n",
    "    * Handling missing values automatically.\n",
    "\n",
    "8. What is the difference between XGBoost and CatBoost\n",
    "    * XGBoost\n",
    "        - Handling Categorical Data Requires encoding (One-Hot, Label)\n",
    "        - Speed Fast\n",
    "        - Implementation Complexity\tComplex\n",
    "        - Can handle missing values automatically\n",
    "\n",
    "    * CatBoost\n",
    "        - Handling Categorical Data Uses ordered boosting (handles categorical data efficiently)\n",
    "        - Speed Faster for categorical features\n",
    "        - Implementation Complexity\tSimpler for categorical-heavy data\n",
    "\n",
    "\n",
    "9. What are some real-world applications of Boosting techniques\n",
    "    * Finance: Fraud detection, credit scoring.\n",
    "    * Healthcare: Disease prediction, diagnosis automation.\n",
    "    * E-commerce: Product recommendation, customer segmentation.\n",
    "    * Cybersecurity: Intrusion detection systems.\n",
    "    * NLP: Sentiment analysis, spam detection.\n",
    "\n",
    "10. How does regularization help in XGBoost\n",
    "    * Regularization (L1 and L2 penalties) prevents overfitting by:\n",
    "        - Shrinking leaf values to avoid overly complex trees.\n",
    "        - Reducing model variance and improving generalization.\n",
    "\n",
    "\n",
    "11. What are some hyperparameters to tune in Gradient Boosting models\n",
    "    * Learning Rate (eta): Controls step size in optimization.\n",
    "    * Number of Estimators: Number of boosting rounds.\n",
    "    * Max Depth: Controls tree complexity.\n",
    "    * Subsample: Fraction of data used per tree (reduces overfitting).\n",
    "    * Colsample_bytree: Feature sampling for diversity.\n",
    "\n",
    "12. What is the concept of Feature Importance in Boosting\n",
    "    * Feature Importance ranks features based on how often they are used for splitting across trees. It helps in:\n",
    "        - Selecting the most informative features.\n",
    "        - Removing redundant or irrelevant features.\n",
    "        - Understanding model decision-making.\n",
    "\n",
    "13. Why is CatBoost efficient for categorical data?\n",
    "    * CatBoost handles categorical data efficiently because:\n",
    "        - It avoids manual encoding (like one-hot encoding).\n",
    "        - Uses ordered boosting, preventing data leakage.\n",
    "        - Efficient categorical encoding through statistical methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier , AdaBoostRegressor , GradientBoostingClassifier , GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier ,  DecisionTreeRegressor\n",
    "from sklearn.datasets import load_iris ,make_regression , make_classification, load_breast_cancer ,load_digits\n",
    "from sklearn.model_selection import train_test_split , GridSearchCV\n",
    "from sklearn.metrics import accuracy_score ,  mean_absolute_error , log_loss, r2_score , f1_score ,  mean_squared_error , confusion_matrix , roc_curve, auc , classification_report\n",
    "from xgboost import XGBClassifier , XGBRegressor\n",
    "from catboost import CatBoostClassifier , CatBoostRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14. Train an AdaBoost Classifier on a sample dataset and print model accuracy\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n"
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Initialize weak learner (Decision Tree Stump)\n",
    "estimator = DecisionTreeClassifier(max_depth=1)\n",
    "\n",
    "# Train AdaBoost Classifier\n",
    "adaboost = AdaBoostClassifier(estimator=estimator, n_estimators=50, learning_rate=1.0, random_state=1)\n",
    "adaboost.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = adaboost.predict(X_test)\n",
    "\n",
    "# Print accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15. Train an AdaBoost Regressor and evaluate performance using Mean Absolute Error (MAE)\n",
    "\n",
    "X, y = make_regression(n_samples=500, n_features=5, noise=0.2, random_state=42)\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize weak learner (Decision Tree Stump)\n",
    "estimator = DecisionTreeRegressor(max_depth=3)\n",
    "\n",
    "# Train AdaBoost Regressor\n",
    "adaboost_regressor = AdaBoostRegressor(estimator=estimator, n_estimators=50, learning_rate=1.0, random_state=42)\n",
    "adaboost_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = adaboost_regressor.predict(X_test)\n",
    "\n",
    "# Evaluate performance using Mean Absolute Error (MAE)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16. Train a Gradient Boosting Classifier on the Breast Cancer dataset and print feature importance\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "X, y = cancer.data, cancer.target\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Train Gradient Boosting Classifier\n",
    "gb_classifier = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=1)\n",
    "gb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = gb_classifier.predict(X_test)\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Get feature importance\n",
    "feature_importance = gb_classifier.feature_importances_\n",
    "\n",
    "# Convert to DataFrame for better visualization\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': cancer.feature_names,\n",
    "    'Importance': feature_importance\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Print Feature Importance\n",
    "print(\"\\nFeature Importance:\")\n",
    "print(feature_importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 17. Train a Gradient Boosting Regressor and evaluate using R-Squared Score\n",
    "\n",
    "X, y = make_regression(n_samples=500, n_features=5, noise=0.2, random_state=1)\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Train Gradient Boosting Regressor\n",
    "gb_regressor = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=1)\n",
    "gb_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = gb_regressor.predict(X_test)\n",
    "\n",
    "# Evaluate performance using R-Squared Score\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"R-Squared Score: {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18. Train an XGBoost Classifier on a dataset and compare accuracy with Gradient Boosting\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "X, y = cancer.data, cancer.target\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Train Gradient Boosting Classifier\n",
    "gb_classifier = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=1)\n",
    "gb_classifier.fit(X_train, y_train)\n",
    "gb_pred = gb_classifier.predict(X_test)\n",
    "gb_accuracy = accuracy_score(y_test, gb_pred)\n",
    "\n",
    "# Train XGBoost Classifier\n",
    "xgb_classifier = XGBClassifier()\n",
    "xgb_classifier.fit(X_train, y_train)\n",
    "xgb_pred = xgb_classifier.predict(X_test)\n",
    "xgb_accuracy = accuracy_score(y_test, xgb_pred)\n",
    "\n",
    "# Print accuracies\n",
    "print(f\"Gradient Boosting Accuracy: {gb_accuracy:.2f}\")\n",
    "print(f\"XGBoost Accuracy: {xgb_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 19. Train a CatBoost Classifier and evaluate using F1-Score\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "X, y = cancer.data, cancer.target\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Train CatBoost Classifier\n",
    "catboost_classifier = CatBoostClassifier(iterations=100, learning_rate=0.1, depth=6, verbose=0, random_state=1)\n",
    "catboost_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = catboost_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate performance using F1-Score\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(f\"F1-Score: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20. Train an XGBoost Regressor and evaluate using Mean Squared Error (MSE)\n",
    "\n",
    "X, y = make_regression(n_samples=500, n_features=5, noise=0.2, random_state=1)\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Train XGBoost Regressor\n",
    "xgb_regressor = XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=1)\n",
    "xgb_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = xgb_regressor.predict(X_test)\n",
    "\n",
    "# Evaluate performance using Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 21. Train an AdaBoost Classifier and visualize feature importance\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "X, y = cancer.data, cancer.target\n",
    "feature_names = cancer.feature_names\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Train AdaBoost Classifier\n",
    "adaboost_classifier = AdaBoostClassifier(\n",
    "    estimator=DecisionTreeClassifier(max_depth=1),\n",
    "    n_estimators=50,\n",
    "    learning_rate=1.0,\n",
    "    random_state=1\n",
    ")\n",
    "adaboost_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importance\n",
    "feature_importance = adaboost_classifier.feature_importances_\n",
    "\n",
    "# Sort feature importance for better visualization\n",
    "sorted_idx = np.argsort(feature_importance)[::-1]\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(feature_names)), feature_importance[sorted_idx], align=\"center\")\n",
    "plt.xticks(range(len(feature_names)), np.array(feature_names)[sorted_idx], rotation=90)\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Importance Score\")\n",
    "plt.title(\"Feature Importance in AdaBoost Classifier\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 22. Train a Gradient Boosting Regressor and plot learning curves\n",
    "\n",
    "X, y = make_regression(n_samples=500, n_features=5, noise=0.2, random_state=1)\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Train Gradient Boosting Regressor\n",
    "gb_regressor = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=1)\n",
    "gb_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Compute training and test errors for each iteration\n",
    "train_errors = []\n",
    "test_errors = []\n",
    "\n",
    "for y_pred_train, y_pred_test in zip(gb_regressor.staged_predict(X_train), gb_regressor.staged_predict(X_test)):\n",
    "    train_errors.append(mean_squared_error(y_train, y_pred_train))\n",
    "    test_errors.append(mean_squared_error(y_test, y_pred_test))\n",
    "\n",
    "# Plot learning curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(train_errors) + 1), train_errors, label=\"Training Error\", linestyle=\"--\", marker=\"o\")\n",
    "plt.plot(range(1, len(test_errors) + 1), test_errors, label=\"Testing Error\", linestyle=\"-\", marker=\"s\")\n",
    "plt.xlabel(\"Number of Trees (Estimators)\")\n",
    "plt.ylabel(\"Mean Squared Error (MSE)\")\n",
    "plt.title(\"Learning Curves: Gradient Boosting Regressor\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 23. Train an XGBoost Classifier and visualize feature importance\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "X, y = cancer.data, cancer.target\n",
    "feature_names = cancer.feature_names\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Train XGBoost Classifier\n",
    "xgb_classifier = XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, use_label_encoder=False, eval_metric='logloss', random_state=1)\n",
    "xgb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict and compute accuracy\n",
    "y_pred = xgb_classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Get feature importance scores\n",
    "feature_importance = xgb_classifier.feature_importances_\n",
    "\n",
    "# Sort feature importance for better visualization\n",
    "sorted_idx = np.argsort(feature_importance)[::-1]\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(feature_names)), feature_importance[sorted_idx], align=\"center\")\n",
    "plt.xticks(range(len(feature_names)), np.array(feature_names)[sorted_idx], rotation=90)\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Importance Score\")\n",
    "plt.title(\"Feature Importance in XGBoost Classifier\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 24. Train a CatBoost Classifier and plot the confusion matrix\n",
    "\n",
    "# Train CatBoost Classifier\n",
    "catboost_classifier = CatBoostClassifier(iterations=100, learning_rate=0.1, depth=6, verbose=0, random_state=1)\n",
    "catboost_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = catboost_classifier.predict(X_test)\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot confusion matrix using Seaborn\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Malignant', 'Benign'], yticklabels=['Malignant', 'Benign'])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix - CatBoost Classifier\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 25. Train an AdaBoost Classifier with different numbers of estimators and compare accuracy\n",
    "\n",
    "# Different numbers of estimators to test\n",
    "n_estimators_list = [10, 50, 100, 200, 500]\n",
    "accuracy_scores = []\n",
    "\n",
    "# Train AdaBoost Classifier with different numbers of estimators\n",
    "for n in n_estimators_list:\n",
    "    adaboost_classifier = AdaBoostClassifier(\n",
    "        estimator=DecisionTreeClassifier(max_depth=1),\n",
    "        n_estimators=n,\n",
    "        learning_rate=1.0,\n",
    "        random_state=1\n",
    "    )\n",
    "    adaboost_classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict and compute accuracy\n",
    "    y_pred = adaboost_classifier.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracy_scores.append(accuracy)\n",
    "    print(f\"Estimators: {n}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Plot accuracy vs. number of estimators\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(n_estimators_list, accuracy_scores, marker='o', linestyle='-')\n",
    "plt.xlabel(\"Number of Estimators\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"AdaBoost Classifier: Accuracy vs. Number of Estimators\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 26. Train a Gradient Boosting Classifier and visualize the ROC curve\n",
    "\n",
    "gb_classifier = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=1)\n",
    "gb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Get predicted probabilities\n",
    "y_scores = gb_classifier.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Compute ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_scores)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC Curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')  # Diagonal reference line\n",
    "plt.xlabel(\"False Positive Rate (FPR)\")\n",
    "plt.ylabel(\"True Positive Rate (TPR)\")\n",
    "plt.title(\"ROC Curve - Gradient Boosting Classifier\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 27. Train an XGBoost Regressor and tune the learning rate using GridSearchCV\n",
    "\n",
    "X, y = make_regression(n_samples=500, n_features=5, noise=0.2, random_state=1)\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Define XGBoost Regressor\n",
    "xgb_regressor = XGBRegressor(n_estimators=100, max_depth=3, random_state=1)\n",
    "\n",
    "# Define parameter grid for learning rate tuning\n",
    "param_grid = {'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3]}\n",
    "\n",
    "# Use GridSearchCV for hyperparameter tuning\n",
    "grid_search = GridSearchCV(xgb_regressor, param_grid, cv=5, scoring='neg_mean_squared_error', verbose=1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best learning rate\n",
    "best_lr = grid_search.best_params_['learning_rate']\n",
    "print(f\"Best Learning Rate: {best_lr}\")\n",
    "\n",
    "# Train final model with best learning rate\n",
    "best_xgb_regressor = XGBRegressor(n_estimators=100, learning_rate=best_lr, max_depth=3, random_state=1)\n",
    "best_xgb_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate performance\n",
    "y_pred = best_xgb_regressor.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error (MSE) with Best Learning Rate ({best_lr}): {mse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 28. Train a CatBoost Classifier on an imbalanced dataset and compare performance with class weighting\n",
    "\n",
    "X, y = make_classification(n_samples=5000, n_features=10, weights=[0.9, 0.1], random_state=1)\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1, stratify=y)\n",
    "\n",
    "# Train CatBoost Classifier without class weighting\n",
    "catboost_no_weights = CatBoostClassifier(iterations=100, learning_rate=0.1, depth=6, verbose=0, random_state=1)\n",
    "catboost_no_weights.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_no_weights = catboost_no_weights.predict(X_test)\n",
    "print(\"Performance without Class Weighting:\")\n",
    "print(classification_report(y_test, y_pred_no_weights))\n",
    "\n",
    "# Train CatBoost Classifier with class weighting\n",
    "class_weights = {0: 1, 1: 9}  # Adjusting for the 90-10 imbalance\n",
    "catboost_with_weights = CatBoostClassifier(iterations=100, learning_rate=0.1, depth=6, class_weights=class_weights, verbose=0, random_state=1)\n",
    "catboost_with_weights.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_with_weights = catboost_with_weights.predict(X_test)\n",
    "print(\"\\nPerformance with Class Weighting:\")\n",
    "print(classification_report(y_test, y_pred_with_weights))\n",
    "\n",
    "# Compute confusion matrices\n",
    "cm_no_weights = confusion_matrix(y_test, y_pred_no_weights)\n",
    "cm_with_weights = confusion_matrix(y_test, y_pred_with_weights)\n",
    "\n",
    "# Plot confusion matrices\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "sns.heatmap(cm_no_weights, annot=True, fmt=\"d\", cmap=\"Blues\", ax=axes[0])\n",
    "axes[0].set_title(\"Confusion Matrix - No Class Weighting\")\n",
    "axes[0].set_xlabel(\"Predicted\")\n",
    "axes[0].set_ylabel(\"Actual\")\n",
    "\n",
    "sns.heatmap(cm_with_weights, annot=True, fmt=\"d\", cmap=\"Reds\", ax=axes[1])\n",
    "axes[1].set_title(\"Confusion Matrix - With Class Weighting\")\n",
    "axes[1].set_xlabel(\"Predicted\")\n",
    "axes[1].set_ylabel(\"Actual\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 29. Train an AdaBoost Classifier and analyze the effect of different learning rates\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "X, y = cancer.data, cancer.target\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Different learning rates to test\n",
    "learning_rates = [0.001, 0.01, 0.1, 0.5, 1.0, 2.0]\n",
    "accuracy_scores = []\n",
    "\n",
    "# Train AdaBoost Classifier with different learning rates\n",
    "for lr in learning_rates:\n",
    "    adaboost_classifier = AdaBoostClassifier(\n",
    "        estimator=DecisionTreeClassifier(max_depth=1),\n",
    "        n_estimators=100,\n",
    "        learning_rate=lr,\n",
    "        random_state=1\n",
    "    )\n",
    "    adaboost_classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict and compute accuracy\n",
    "    y_pred = adaboost_classifier.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracy_scores.append(accuracy)\n",
    "    print(f\"Learning Rate: {lr}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Plot accuracy vs. learning rate\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(learning_rates, accuracy_scores, marker='o', linestyle='-')\n",
    "plt.xlabel(\"Learning Rate\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"AdaBoost Classifier: Accuracy vs. Learning Rate\")\n",
    "plt.xscale(\"log\")  # Log scale for better visualization\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 30. Train an XGBoost Classifier for multi-class classification and evaluate using log-loss\n",
    "\n",
    "digits = load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1, stratify=y)\n",
    "\n",
    "# Train XGBoost Classifier for multi-class classification\n",
    "xgb_classifier = xgb.XGBClassifier(\n",
    "    objective=\"multi:softprob\",  # Softmax for multi-class classification\n",
    "    num_class=10,  # Number of classes\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    random_state=1\n",
    ")\n",
    "xgb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict class probabilities\n",
    "y_pred_proba = xgb_classifier.predict_proba(X_test)\n",
    "\n",
    "# Compute log-loss\n",
    "logloss = log_loss(y_test, y_pred_proba)\n",
    "print(f\"Log-Loss: {logloss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
