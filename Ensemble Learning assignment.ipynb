{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theoretical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Can we use Bagging for regression problems  \n",
    "    * Yes, Bagging  can be used for regression problems. \n",
    "\n",
    "2. What is the difference between multiple model training and single model training  \n",
    "    * Single model training involves training just one model on the entire dataset, relying on it to make predictions.\n",
    "    * Multiple model training (ensemble learning) involves training multiple models and combining their predictions to improve accuracy, stability, and  generalization. Examples include Bagging, Boosting, and Stacking.\n",
    "\n",
    "\n",
    "3. Explain the concept of feature randomness in Random Forest\n",
    "\n",
    "    - Feature randomness in Random Forest refers to the process where, at each split of a decision tree, only a random subset of the total features is considered. This helps reduce overfitting and increases diversity among trees, improving overall model performance.\n",
    "\n",
    "4. What is OOB (Out-of-Bag) Score\n",
    "\n",
    "    - Mean Decrease in Impurity (MDI): Measures how much each feature reduces impurity (e.g., Gini impurity or entropy) across all trees.\n",
    "    - Permutation Importance: Measures the decrease in model performance (e.g., accuracy or RMSE) when the values of a feature are randomly shuffled.\n",
    "    \n",
    "5. How can you measure the importance of features in a Random Forest model\n",
    "\n",
    "    - Mean Decrease in Impurity (MDI) or Permutation Importance\n",
    "    - Gini importance or feature importance scores can be calculated using the MDI or permutation importance method.\n",
    "\n",
    "\n",
    "6. Explain the working principle of a Bagging Classifier\n",
    "\n",
    "    - Creating multiple bootstrap samples from the original dataset.\n",
    "    - Training a separate base classifier (e.g., decision tree) on each bootstrap sample.\n",
    "    - Aggregating the predictions of all classifiers, typically using majority voting.\n",
    "\n",
    "\n",
    "7. How do you evaluate a Bagging Classifierâ€™s performance\n",
    "\n",
    "    - Accuracy, Precision, Recall, and F1-score for classification problems.\n",
    "    - OOB Score for internal validation.\n",
    "    - Cross-validation to get a reliable estimate of model performance.\n",
    "    - ROC-AUC score for imbalanced datasets.\n",
    "\n",
    "8. How does a Bagging Regressor work\n",
    "\n",
    "    - A Bagging Regressor follows the same principle as a Bagging Classifier but for regression:\n",
    "        - Creates multiple bootstrap samples.\n",
    "        - Trains separate regression models (e.g., Decision Trees).\n",
    "        - Aggregates predictions by averaging the outputs of all models.\n",
    "\n",
    "9. What is the main advantage of ensemble techniques\n",
    "\n",
    "    - The main advantage is higher accuracy and better generalization. By combining multiple models, ensemble techniques reduce variance, mitigate overfitting, and improve prediction robustness.\n",
    "\n",
    "10. What is the main challenge of ensemble methods\n",
    "\n",
    "    - The main challenge is computational complexity and interpretability. Training multiple models requires more computational resources, and interpreting ensemble models can be more difficult than understanding a single decision tree or regression model.\n",
    "\n",
    "11. Explain the key idea behind ensemble techniques\n",
    "    \n",
    "    - Ensemble techniques combine multiple models to improve prediction accuracy, reduce overfitting, and increase robustness. The idea is that a group of diverse models can make better decisions than a single model.\n",
    "\n",
    "\n",
    "12. What is a Random Forest Classifier\n",
    "\n",
    "    - A Random Forest Classifier is an ensemble learning method that builds multiple decision trees and combines their predictions using majority voting. It introduces randomness by selecting different data samples (bootstrapping) and considering only a subset of features at each split.\n",
    "\n",
    "\n",
    "13. What are the main types of ensemble techniques\n",
    "\n",
    "    - Bagging (Bootstrap Aggregating): Reduces variance by training multiple models on different subsets of data and averaging predictions.\n",
    "    - Boosting: Reduces bias by training models sequentially, where each new model corrects the errors of the previous one.\n",
    "\n",
    "14. What is ensemble learning in machine learning\n",
    "\n",
    "    - Ensemble learning is a technique where multiple models (often of the same or different types) are combined to make better predictions than a single model\n",
    "\n",
    "15. When should we avoid using ensemble methods\n",
    "\n",
    "    - When computational resources are limited, as ensembles require more training time.\n",
    "    - When interpretability is crucial, since ensembles are harder to explain than single models.\n",
    "    - When a single model performs well enough, making an ensemble unnecessary.\n",
    "\n",
    "\n",
    "16. How does Bagging help in reducing overfitting\n",
    "\n",
    "    - Bagging reduces overfitting by training multiple models on different bootstrap samples and averaging their predictions. This helps smooth out the variance in individual models.\n",
    "\n",
    "17. Why is Random Forest better than a single Decision Tree\n",
    "\n",
    "    - More robust: Reduces overfitting by averaging multiple trees.\n",
    "    - More stable: Less sensitive to noise in data.\n",
    "    - More accurate: Handles large datasets better and provides feature importance.\n",
    "\n",
    "18. What is the role of bootstrap sampling in Bagging\n",
    "\n",
    "    - Bootstrap sampling randomly selects subsets of the dataset (with replacement) to train each base model. This ensures diversity among models, leading to better generalization.\n",
    "\n",
    "19. What are some real-world applications of ensemble techniques\n",
    "\n",
    "    - Finance: Fraud detection and credit risk assessment.\n",
    "    - Healthcare: Disease prediction and medical diagnosis.\n",
    "    - Image Recognition: Facial recognition and object detection.\n",
    "    - Natural Language Processing: Sentiment analysis and spam filtering.\n",
    "    - Recommendation Systems: Movie, product, and content recommendations.\n",
    "\n",
    "20. What is the difference between Bagging and Boosting?\n",
    "\n",
    "    - Bagging: Reduces variance by training multiple models independently on different random subsets of data and averaging their predictions (e.g., Random Forest).\n",
    "    - Boosting: Reduces bias by training models sequentially, where each model learns from the errors of the previous one (e.g., AdaBoost, Gradient Boosting).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier , BaggingRegressor , RandomForestClassifier ,RandomForestRegressor , StackingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier , DecisionTreeRegressor\n",
    "from sklearn.datasets import make_classification , make_regression\n",
    "from sklearn.model_selection import train_test_split ,  GridSearchCV , cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score ,  mean_squared_error ,  roc_auc_score , confusion_matrix ,precision_score, recall_score, f1_score , roc_curve ,  precision_recall_curve, auc\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 21. Train a Bagging Classifier using Decision Trees on a sample dataset and print model accuracy\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_features=20, random_state=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Train a Bagging Classifier using Decision Trees\n",
    "bagging_clf = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=50, random_state=1)\n",
    "bagging_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate accuracy\n",
    "y_pred = bagging_clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print accuracy\n",
    "print(f\"Bagging Classifier Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 22. Train a Bagging Regressor using Decision Trees and evaluate using Mean Squared Error (MSE)\n",
    "\n",
    "X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Train a Bagging Regressor using Decision Trees\n",
    "bagging_reg = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=50, random_state=1)\n",
    "bagging_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate MSE\n",
    "y_pred = bagging_reg.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Print MSE\n",
    "print(f\"Bagging Regressor Mean Squared Error: {mse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 23. Train a Random Forest Classifier on the Breast Cancer dataset and print feature importance scores\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "\n",
    "# Load the Breast Cancer dataset\n",
    "cancer_data = load_breast_cancer()\n",
    "X, y = cancer_data.data, cancer_data.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Train a Random Forest Classifier\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=1)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importance scores\n",
    "feature_importances = rf_clf.feature_importances_\n",
    "\n",
    "# Print feature importance scores with feature names\n",
    "print(\"Feature Importances:\")\n",
    "for feature, importance in zip(cancer_data.feature_names, feature_importances):\n",
    "    print(f\"{feature}: {importance:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 24. Train a Random Forest Regressor and compare its performance with a single Decision Tree\n",
    "\n",
    "X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Train a Decision Tree Regressor\n",
    "dt_reg = DecisionTreeRegressor()\n",
    "dt_reg.fit(X_train, y_train)\n",
    "\n",
    "# Train a Random Forest Regressor\n",
    "rf_reg = RandomForestRegressor()\n",
    "rf_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate MSE for both models\n",
    "dt_mse = mean_squared_error(y_test, dt_reg.predict(X_test))\n",
    "rf_mse = mean_squared_error(y_test, rf_reg.predict(X_test))\n",
    "\n",
    "# Print the MSE scores\n",
    "print(f\"Decision Tree Regressor MSE: {dt_mse:.2f}\")\n",
    "print(f\"Random Forest Regressor MSE: {rf_mse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 25 Compute the Out-of-Bag (OOB) Score for a Random Forest Classifier\n",
    "\n",
    "cancer_data = load_breast_cancer()\n",
    "X, y = cancer_data.data, cancer_data.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Train a Random Forest Classifier with OOB score enabled\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=1)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# Print the Out-of-Bag (OOB) score\n",
    "print(f\"Out-of-Bag (OOB) Score: {rf_clf.oob_score_:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 26 Train a Bagging Classifier using SVM as a base estimator and print accuracy\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_features=20, random_state=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Train a Bagging Classifier using SVM as the base estimator\n",
    "bagging_svm = BaggingClassifier(estimator=SVC(), n_estimators=50, random_state=1)\n",
    "bagging_svm.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate accuracy\n",
    "y_pred = bagging_svm.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print accuracy\n",
    "print(f\"Bagging Classifier with SVM Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 27 Train a Random Forest Classifier with different numbers of trees and compare accuracy\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_features=20, random_state=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# List of different numbers of trees to test\n",
    "n_trees_list = [10, 50, 100, 200]\n",
    "\n",
    "# Train Random Forest classifiers with different numbers of trees and compare accuracy\n",
    "for n_trees in n_trees_list:\n",
    "    rf_clf = RandomForestClassifier(n_estimators=n_trees, random_state=1)\n",
    "    rf_clf.fit(X_train, y_train)\n",
    "    y_pred = rf_clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Random Forest with {n_trees} trees - Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 28. Train a Bagging Classifier using Logistic Regression as a base estimator and print AUC score\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_features=20, random_state=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Train a Bagging Classifier using Logistic Regression as the base estimator\n",
    "bagging_lr = BaggingClassifier(estimator=LogisticRegression(), n_estimators=50, random_state=1)\n",
    "bagging_lr.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities for AUC calculation\n",
    "y_prob = bagging_lr.predict_proba(X_test)[:, 1]  # Get probability of the positive class\n",
    "\n",
    "# Compute and print AUC score\n",
    "auc_score = roc_auc_score(y_test, y_prob)\n",
    "print(f\"Bagging Classifier with Logistic Regression - AUC Score: {auc_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 29. Train a Random Forest Regressor and analyze feature importance scores\n",
    "\n",
    "X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Train a Random Forest Regressor\n",
    "rf_reg = RandomForestRegressor(n_estimators=100, random_state=1)\n",
    "rf_reg.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importance scores\n",
    "feature_importances = rf_reg.feature_importances_\n",
    "\n",
    "# Print feature importance scores\n",
    "print(\"Feature Importances:\")\n",
    "for i, importance in enumerate(feature_importances):\n",
    "    print(f\"Feature {i + 1}: {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 30 Train an ensemble model using both Bagging and Random Forest and compare accuracy.\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_features=20, random_state=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Train a Bagging Classifier using Decision Trees\n",
    "bagging_clf = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=50, random_state=1)\n",
    "bagging_clf.fit(X_train, y_train)\n",
    "bagging_pred = bagging_clf.predict(X_test)\n",
    "bagging_accuracy = accuracy_score(y_test, bagging_pred)\n",
    "\n",
    "# Train a Random Forest Classifier\n",
    "rf_clf = RandomForestClassifier(n_estimators=50, random_state=1)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "rf_pred = rf_clf.predict(X_test)\n",
    "rf_accuracy = accuracy_score(y_test, rf_pred)\n",
    "\n",
    "# Print accuracy of both models\n",
    "print(f\"Bagging Classifier Accuracy: {bagging_accuracy:.4f}\")\n",
    "print(f\"Random Forest Classifier Accuracy: {rf_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 31. Train a Random Forest Classifier and tune hyperparameters using GridSearchCV\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_features=20, random_state= 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Define the Random Forest Classifier\n",
    "rf_clf = RandomForestClassifier()\n",
    "\n",
    "# Define hyperparameters for tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],  \n",
    "    'max_depth': [5, 10, 20],  \n",
    "    'min_samples_split': [2, 5, 10], \n",
    "    'min_samples_leaf': [1, 2, 4] \n",
    "}\n",
    "\n",
    "# Perform Grid Search with Cross-Validation\n",
    "grid_search = GridSearchCV(estimator=rf_clf, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model\n",
    "best_rf = grid_search.best_estimator_\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = best_rf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print best hyperparameters and accuracy\n",
    "print(f\"Best Hyperparameters: {grid_search.best_params_}\")\n",
    "print(f\"Random Forest Classifier Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 32. Train a Bagging Regressor with different numbers of base estimators and compare performance\n",
    "\n",
    "X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# List of different numbers of base estimators to test\n",
    "n_estimators_list = [10, 50, 100, 200]\n",
    "\n",
    "# Train Bagging Regressors with different numbers of base estimators and compare performance\n",
    "for n_estimators in n_estimators_list:\n",
    "    bagging_reg = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=n_estimators, random_state=1)\n",
    "    bagging_reg.fit(X_train, y_train)\n",
    "    y_pred = bagging_reg.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    print(f\"Bagging Regressor with {n_estimators} estimators - MSE: {mse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 33. Train a Random Forest Classifier and analyze misclassified samples\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_features=20, random_state=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Train a Random Forest Classifier\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=1)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = rf_clf.predict(X_test)\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Random Forest Classifier Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Identify misclassified samples\n",
    "misclassified_indices = np.where(y_test != y_pred)[0]\n",
    "misclassified_samples = X_test[misclassified_indices]\n",
    "\n",
    "# Convert misclassified samples to a DataFrame for better readability\n",
    "misclassified_df = pd.DataFrame(misclassified_samples, columns=[f\"Feature_{i}\" for i in range(X.shape[1])])\n",
    "misclassified_df['Actual Label'] = y_test[misclassified_indices]\n",
    "misclassified_df['Predicted Label'] = y_pred[misclassified_indices]\n",
    "\n",
    "# Display the first few misclassified samples\n",
    "print(\"\\nMisclassified Samples:\")\n",
    "print(misclassified_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 34. Train a Bagging Classifier and compare its performance with a single Decision Tree Classifier\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_features=20, random_state=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Train a single Decision Tree Classifier\n",
    "dt_clf = DecisionTreeClassifier(random_state=1)\n",
    "dt_clf.fit(X_train, y_train)\n",
    "dt_pred = dt_clf.predict(X_test)\n",
    "dt_accuracy = accuracy_score(y_test, dt_pred)\n",
    "\n",
    "# Train a Bagging Classifier using Decision Tree as the base estimator\n",
    "bagging_clf = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=50, random_state=1)\n",
    "bagging_clf.fit(X_train, y_train)\n",
    "bagging_pred = bagging_clf.predict(X_test)\n",
    "bagging_accuracy = accuracy_score(y_test, bagging_pred)\n",
    "\n",
    "# Print accuracy of both models\n",
    "print(f\"Decision Tree Classifier Accuracy: {dt_accuracy:.4f}\")\n",
    "print(f\"Bagging Classifier Accuracy: {bagging_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 35. Train a Random Forest Classifier and visualize the confusion matrix\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_features=20, random_state=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Train a Random Forest Classifier\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=1)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = rf_clf.predict(X_test)\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Random Forest Classifier Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Visualize confusion matrix using seaborn\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'])\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 36. Train a Stacking Classifier using Decision Trees, SVM, and Logistic Regression, and compare accuracy\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_features=20, random_state=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Define base estimators\n",
    "estimators = [\n",
    "    ('decision_tree', DecisionTreeClassifier(random_state=1)),\n",
    "    ('svm', SVC(probability=True, random_state=1))\n",
    "]\n",
    "\n",
    "# Define the Stacking Classifier with Logistic Regression as the final estimator\n",
    "stacking_clf = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression(), cv=5)\n",
    "stacking_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict and compute accuracy\n",
    "y_pred = stacking_clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print accuracy\n",
    "print(f\"Stacking Classifier Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 37. Train a Random Forest Classifier and print the top 5 most important features\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_features=20, random_state=1)\n",
    "feature_names = [f\"Feature_{i}\" for i in range(X.shape[1])]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Train a Random Forest Classifier\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=1)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importance scores\n",
    "feature_importances = rf_clf.feature_importances_\n",
    "\n",
    "# Create a DataFrame for better readability\n",
    "importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Print the top 5 most important features\n",
    "print(\"Top 5 Most Important Features:\")\n",
    "print(importance_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 38. Train a Bagging Classifier and evaluate performance using Precision, Recall, and F1-score\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_features=20, random_state=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Train a Bagging Classifier using Decision Tree as the base estimator\n",
    "bagging_clf = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=50, random_state=1)\n",
    "bagging_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = bagging_clf.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Bagging Classifier Performance:\")\n",
    "print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1-score:  {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 39. Train a Random Forest Classifier and analyze the effect of max_depth on accuracy\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_features=20, random_state=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Define different values for max_depth\n",
    "max_depth_values = [2, 5, 10, 20, None]  # None means unlimited depth\n",
    "\n",
    "# Store accuracies for analysis\n",
    "accuracies = []\n",
    "\n",
    "# Train and evaluate Random Forest models with different max_depth values\n",
    "for max_depth in max_depth_values:\n",
    "    rf_clf = RandomForestClassifier(n_estimators=100, max_depth=max_depth, random_state=1)\n",
    "    rf_clf.fit(X_train, y_train)\n",
    "    y_pred = rf_clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "    print(f\"Max Depth: {max_depth}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Plot the effect of max_depth on accuracy\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot([str(md) for md in max_depth_values], accuracies, marker='o', linestyle='-', color='b')\n",
    "plt.xlabel(\"Max Depth\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Effect of max_depth on Random Forest Accuracy\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 40. Train a Bagging Regressor using different base estimators (DecisionTree and KNeighbors) and compareperformance\n",
    "\n",
    "X, y = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Define base estimators\n",
    "base_estimators = {\n",
    "    \"Decision Tree\": DecisionTreeRegressor(),\n",
    "    \"K-Neighbors\": KNeighborsRegressor()\n",
    "}\n",
    "\n",
    "# Store results\n",
    "mse_scores = {}\n",
    "\n",
    "# Train and evaluate Bagging Regressors with different base estimators\n",
    "for name, base_estimator in base_estimators.items():\n",
    "    bagging_regressor = BaggingRegressor()\n",
    "    bagging_regressor.fit(X_train, y_train)\n",
    "    y_pred = bagging_regressor.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mse_scores[name] = mse\n",
    "    print(f\"Bagging Regressor ({name}) - Mean Squared Error: {mse:.4f}\")\n",
    "\n",
    "# Plot comparison of MSE\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar(mse_scores.keys(), mse_scores.values(), color=['blue', 'green'])\n",
    "plt.ylabel(\"Mean Squared Error (MSE)\")\n",
    "plt.title(\"Comparison of Bagging Regressors with Different Base Estimators\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 41. Train a Random Forest Classifier and evaluate its performance using ROC-AUC Score\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_features=20, random_state=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Train a Random Forest Classifier\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=1)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities for ROC-AUC\n",
    "y_probs = rf_clf.predict_proba(X_test)[:, 1]  # Get probability scores for the positive class\n",
    "\n",
    "# Compute ROC-AUC score\n",
    "roc_auc = roc_auc_score(y_test, y_probs)\n",
    "\n",
    "# Compute ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_probs)\n",
    "\n",
    "# Print ROC-AUC score\n",
    "print(f\"Random Forest Classifier ROC-AUC Score: {roc_auc:.4f}\")\n",
    "\n",
    "# Plot the ROC Curve\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(fpr, tpr, color='blue', label=f\"ROC Curve (AUC = {roc_auc:.4f})\")\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='red', label=\"Random Guess\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve for Random Forest Classifier\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 42. Train a Bagging Classifier and evaluate its performance using cross-validatio\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_features=20, random_state=1)\n",
    "\n",
    "# Define a Bagging Classifier with Decision Tree as the base estimator\n",
    "bagging_clf = BaggingClassifier()\n",
    "\n",
    "# Perform cross-validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "cv_scores = cross_val_score(bagging_clf, X, y, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print cross-validation results\n",
    "print(f\"Cross-Validation Accuracy Scores: {cv_scores}\")\n",
    "print(f\"Mean Accuracy: {cv_scores.mean():.4f}\")\n",
    "print(f\"Standard Deviation: {cv_scores.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 43. Train a Random Forest Classifier and plot the Precision-Recall curv\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_features=20, random_state=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Train a Random Forest Classifier\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=1)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities for Precision-Recall Curve\n",
    "y_probs = rf_clf.predict_proba(X_test)[:, 1]  # Get probability scores for the positive class\n",
    "\n",
    "# Compute Precision-Recall curve\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_probs)\n",
    "\n",
    "# Compute AUC (Area Under Curve) for Precision-Recall\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "# Plot the Precision-Recall Curve\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(recall, precision, color='blue', label=f\"PR Curve (AUC = {pr_auc:.4f})\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve for Random Forest Classifier\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Print PR AUC Score\n",
    "print(f\"Precision-Recall AUC Score: {pr_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 44. Train a Stacking Classifier with Random Forest and Logistic Regression and compare accuracy\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_features=20, random_state=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Define base estimators\n",
    "base_estimators = [\n",
    "    ('random_forest', RandomForestClassifier(n_estimators=100, random_state=1)),\n",
    "    ('svm', SVC(probability=True, random_state=1))\n",
    "]\n",
    "\n",
    "# Define the Stacking Classifier with Logistic Regression as the meta-learner\n",
    "stacking_clf = StackingClassifier(estimators=base_estimators, final_estimator=LogisticRegression(), cv=5)\n",
    "\n",
    "# Train the Stacking Classifier\n",
    "stacking_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = stacking_clf.predict(X_test)\n",
    "\n",
    "# Compute accuracy\n",
    "stacking_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Stacking Classifier Accuracy: {stacking_accuracy:.4f}\")\n",
    "\n",
    "# Train and compare individual models\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=1)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "rf_accuracy = accuracy_score(y_test, rf_clf.predict(X_test))\n",
    "\n",
    "lr_clf = LogisticRegression()\n",
    "lr_clf.fit(X_train, y_train)\n",
    "lr_accuracy = accuracy_score(y_test, lr_clf.predict(X_test))\n",
    "\n",
    "# Print individual model accuracies\n",
    "print(f\"Random Forest Accuracy: {rf_accuracy:.4f}\")\n",
    "print(f\"Logistic Regression Accuracy: {lr_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 45. Train a Bagging Regressor with different levels of bootstrap samples and compare performance\n",
    "\n",
    "X, y = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Define different bootstrap sample sizes\n",
    "bootstrap_samples = [0.5, 0.7, 1.0]  # Proportion of samples used for training each base model\n",
    "\n",
    "# Store results\n",
    "mse_scores = {}\n",
    "\n",
    "# Train and evaluate Bagging Regressors with different bootstrap sample sizes\n",
    "for sample_size in bootstrap_samples:\n",
    "    bagging_regressor = BaggingRegressor(\n",
    "        estimator=DecisionTreeRegressor(),\n",
    "        n_estimators=50,\n",
    "        max_samples=sample_size,  # Define bootstrap sample size\n",
    "        random_state=1\n",
    "    )\n",
    "    bagging_regressor.fit(X_train, y_train)\n",
    "    y_pred = bagging_regressor.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mse_scores[sample_size] = mse\n",
    "    print(f\"Bagging Regressor (Bootstrap Samples = {sample_size}) - MSE: {mse:.4f}\")\n",
    "\n",
    "# Plot comparison of MSE\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar([str(s) for s in bootstrap_samples], mse_scores.values(), color=['blue', 'green', 'red'])\n",
    "plt.xlabel(\"Bootstrap Sample Size\")\n",
    "plt.ylabel(\"Mean Squared Error (MSE)\")\n",
    "plt.title(\"Comparison of Bagging Regressors with Different Bootstrap Samples\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
