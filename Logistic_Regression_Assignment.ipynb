{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theoretical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What is Logistic Regression, and how does it differ from Linear Regression.\n",
    "\n",
    "* Logistic Regression:  \n",
    "  Logistic Regression is a statistical method used to predict a binary outcome (e.g., yes/no, 0/1, true/false) based on a set of independent variables. It is a type of regression analysis that models the probability of a binary dependent variable. \n",
    "\n",
    "* Linear Regression:   \n",
    "  Linear Regression is a statistical method used to predict a continuous outcome (e.g., height, weight, salary) based on a set of independent variables. It is a type of regression analysis that models the relationship between the dependent variable and the independent variables using a linear equation.\n",
    "\n",
    "\n",
    "2. What is the mathematical equation of Logistic Regression.\n",
    "* Equation of Logistic Regression:  \n",
    "For a given input X (a set of independent variables), Logistic Regression models the probability P(Y=1‚à£X) using the sigmoid function:  \n",
    "\n",
    "P(Y=1‚à£X)=1 /  1+e‚àí(Œ≤0+Œ≤1X1+Œ≤2X2+...+Œ≤ nXn)1\n",
    "‚Äã\n",
    "where:\n",
    "- P(Y=1‚à£X) is the probability that the dependent variable ùëå is 1 given input ùëã\n",
    "- ùõΩ,ùõΩ1,..,ùõΩùëõŒ≤,Œ≤1,...,Œ≤n are the regression coefficients (weights).\n",
    "- X1,X2 ,...,Xn  are the independent variables (features).\n",
    "- e is Euler‚Äôs number (‚âà 2.718).\n",
    "\n",
    "3. Why do we use the Sigmoid function in Logistic Regression.\n",
    "\n",
    "The Sigmoid function is used in Logistic Regression because it squashes the real-valued linear equation output to a range between 0 and 1. This is essential because the logistic function outputs a probability, which is a meaningful and interpretable measure of the likelihood of an event occurring. Additionally, the sigmoid function has a derivative that makes it suitable for gradient descent-based optimization algorithms, such as Logistic Regression.\n",
    "\n",
    "4. What is the cost function of Logistic Regression.\n",
    "\n",
    "The cost function for Logistic Regression is the cross-entropy loss function, which measures the difference between the predicted probabilities and the actual binary outcomes. The cross-entropy loss function is defined as:\n",
    "\n",
    "* Intuition Behind the Cost Function  \n",
    "- If the actual label is **1** and the predicted probability is **high (close to 1)**, the cost is **low**.\n",
    "- If the actual label is **1** but the predicted probability is **low (close to 0)**, the cost is **high**.\n",
    "- Similarly, for **y = 0**, if the predicted probability is **close to 0**, the cost is **low**, and if it‚Äôs **close to 1**, the cost is **high**.\n",
    "\n",
    "5. What is Regularization in Logistic Regression? Why is it needed.\n",
    "\n",
    "Regularization is a technique used to prevent overfitting in Logistic Regression by adding a penalty term to the loss function. It helps to reduce the complexity of the model and improve its generalization ability by reducing the risk of overfitting. Regularization is needed because Logistic Regression is a linear model, which can lead to high variance and poor generalization performance.\n",
    "\n",
    "6. Explain the difference between Lasso, Ridge, and Elastic Net regression.\n",
    "\n",
    "* Lasso Regression:\n",
    "  Lasso Regression adds a penalty term called L1 regularization to the loss function. It minimizes the loss function by penalizing the sum of absolute values of the regression coefficients (Œ≤) to encourage sparsity (i.e., many regression coefficients becoming zero). This can help to reduce the complexity of the model and improve its interpretability.\n",
    "  \n",
    "* Ridge Regression:\n",
    "  Ridge Regression adds a penalty term called L2 regularization to the loss function. It minimizes the loss function by penalizing the sum of squares of the regression coefficients (Œ≤) to encourage smaller values of the regression coefficients. This can help to reduce the complexity of the model and improve its generalization performance.\n",
    "\n",
    "* Elastic Net Regression:\n",
    "  Elastic Net Regression combines the L1 and L2 regularization techniques to encourage sparsity and balance the trade-off between minimizing the loss function and minimizing the complexity of the model. It uses a combination of the L1 and L2 regularization terms to achieve this balance. This can help to reduce the complexity of the model and improve its generalization performance.\n",
    "\n",
    "7. When should we use Elastic Net instead of Lasso or Ridge.\n",
    "\n",
    "Elastic Net should be used when the dataset contains many features and the model needs to handle high-dimensional data. It can help to balance the trade-off between minimizing the loss function and minimizing the complexity of the model by combining the L1 and L2 regularization terms. It can be more effective than Lasso and Ridge when the dataset is sparse or when the number of features is much larger than the number of observations.\n",
    "\n",
    "8. What is the impact of the regularization parameter (Œª) in Logistic Regression.\n",
    "\n",
    "The regularization parameter (Œª) in Logistic Regression determines the amount of penalty applied to the regression coefficients. A larger value of Œª encourages sparsity (i.e., many regression coefficients becoming zero) and reduces the complexity of the model. A smaller value of Œª allows the model to fit the data more closely and improves its generalization performance. The optimal value of Œª depends on the specific dataset and the problem at hand.\n",
    "\n",
    "9. What are the key assumptions of Logistic Regression.\n",
    "\n",
    "* The dependent variable is binary (i.e., it takes only two values, 0 or 1).\n",
    "* The independent variables are independent of each other (i.e., they do not have a strong correlation).\n",
    "* The independent variables are normally distributed.\n",
    "* The dependent variable and the independent variables are linearly related.\n",
    "* There is no multicollinearity (i.e., the independent variables are not strongly correlated).\n",
    "\n",
    "10. What are some alternatives to Logistic Regression for classification tasks.\n",
    "\n",
    "* Linear Discriminant Analysis (LDA): LDA is a popular classification method that assumes that the dependent variable is normally distributed and the independent variables are linearly related. It is used when the dataset is multivariate and the dependent variable is categorical.\n",
    "* K-Nearest Neighbors (KNN): KNN is a classification method that uses the majority vote of the k nearest neighbors to make predictions. It is effective for classification tasks when the dataset is small and the independent variables are continuous.\n",
    "* Decision Trees: Decision Trees are a classification method that uses a tree-like structure to make predictions. They are effective for classification tasks when the dataset is small and the independent variables are categorical.\n",
    "* Support Vector Machines (SVM): SVM is a classification method that uses a hyperplane to separate the data points into two classes. It is effective for classification tasks when the dataset is small and the independent variables are continuous.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. What are Classification Evaluation Metrics.\n",
    "\n",
    "* Accuracy: The proportion of correct predictions made by the model.\n",
    "* Precision: The proportion of true positives among the predicted positives.\n",
    "* Recall: The proportion of true positives among the actual positives.\n",
    "* F1 Score: The harmonic mean of precision and recall, which is a more balanced measure of the model's performance.\n",
    "* Confusion Matrix: A table that shows the number of true positives, true negatives, false positives, and false negatives for each class.\n",
    "* ROC Curve: A plot that shows the trade-off between the true positive rate (TPR) and the false positive rate (FPR) for different classification thresholds\n",
    "\n",
    "12. How does class imbalance affect Logistic Regression.\n",
    "\n",
    "Class imbalance can affect Logistic Regression by making it more difficult for the model to learn the correct class proportions. This can lead to biased predictions and inaccurate performance metrics. To address class imbalance, we can use techniques such as oversampling the minority class, undersampling the majority class, or using class weights to give more importance to the minority class.\n",
    "\n",
    "\n",
    "13. What is Hyperparameter Tuning in Logistic Regression.\n",
    "\n",
    "Hyperparameter tuning is the process of finding the optimal values for the hyperparameters (e.g., regularization parameter Œª, number of neighbors k in KNN, etc.) in Logistic Regression to improve its performance. Hyperparameter tuning can be done using techniques such as grid search, random search, or Bayesian optimization.\n",
    "\n",
    "14. What are different solvers in Logistic Regression? Which one should be used.\n",
    "\n",
    "* 'liblinear': This solver is used for small datasets and is faster than other solvers.\n",
    "* 'lbfgs': This solver is used for larger datasets and is slower than 'liblinear' but can handle more complex problems.\n",
    "* 'newton-cg': This solver is used for larger datasets and is slower than 'liblinear' but can handle more complex problems. \n",
    "* ' sag': This solver is used for large datasets and is faster than 'liblinear' and 'newton-cg' solvers but can handle more complex problems.\n",
    "* 'saga': This solver is used for large datasets and is faster than 'liblinear', 'newton-cg', and 'lbfgs' solvers but can handle more complex problems.\n",
    "\n",
    "The choice of solver depends on the specific dataset, problem size, and complexity of the problem. It is recommended to experiment with different solvers and choose the one that provides the best performance for your specific use case.\n",
    "\n",
    "15. How is Logistic Regression extended for multiclass classification.\n",
    "\n",
    "Logistic Regression can be extended for multiclass classification using techniques such as one-vs-rest (OvR) or softmax. In OvR, the model is trained for each class against all other classes, and the class with the highest predicted probability is chosen as the predicted class. In softmax, the model is trained for each class against all other classes, and the predicted probabilities are calculated using the softmax function.\n",
    "\n",
    "16. What are the advantages and disadvantages of Logistic Regression.\n",
    "\n",
    "Advantages of Logistic Regression:\n",
    "    * Simple and easy to understand.\n",
    "    * Can handle both binary and multiclass classification problems.\n",
    "    * Can handle missing values and outliers automatically.\n",
    "    * Can handle regularization automatically.\n",
    "    \n",
    "Disadvantages of Logistic Regression:\n",
    "    * Sensitive to outliers and missing values.\n",
    "    * Does not provide probability estimates for each class.\n",
    "    * Does not handle non-linear relationships between the independent variables and the dependent variable.\n",
    "    * Does not handle feature scaling automatically.\n",
    "\n",
    "17. What are some use cases of Logistic Regression.\n",
    "\n",
    "Logistic Regression can be used for binary classification tasks, such as predicting whether a customer will subscribe to a marketing campaign or not. It can also be used for multiclass classification tasks, such as predicting the species of an iris flower based on its features. Logistic Regression can be used in combination with other classification algorithms, such as Naive Bayes, Decision Trees, and Support Vector Machines, to improve the overall performance of the model.\n",
    "\n",
    "18. What is the difference between Softmax Regression and Logistic Regression.\n",
    "\n",
    "Softmax Regression is a generalization of Logistic Regression that can handle multiclass classification problems. In Softmax Regression, the predicted probabilities for each class are calculated using the softmax function, which ensures that the probabilities sum up to 1 for each instance. Softmax Regression can be used when the dependent variable has more than two categories.\n",
    "\n",
    "Logistic Regression is simpler and easier to understand than Softmax Regression, but it may not be as effective for multiclass classification problems. In Logistic Regression, the predicted probabilities for each class are calculated using the logistic function, which can be difficult to interpret.\n",
    "\n",
    "19. How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification.\n",
    "\n",
    "OvR is a method that trains multiple binary classifiers, one for each class, and uses the predicted probabilities from the classifiers to make the final prediction. Softmax is a method that trains a single classifier for each class, and uses the predicted probabilities from the classifier to make the final prediction.\n",
    "\n",
    "OvR can be more efficient and easier to interpret compared to Softmax, as it does not require calculating the predicted probabilities for each class. However, it may not be as effective for multiclass classification problems with many categories, as it may not be able to capture the relationship between the independent variables and the dependent variable for each class.\n",
    "\n",
    "Softmax can be more effective for multiclass classification problems with many categories, as it can capture the relationship between the independent variables and the dependent variable for each class. However, it may be more difficult to interpret compared to OvR, as it requires calculating the predicted probabilities for each class.\n",
    "\n",
    "It is recommended to experiment with both OvR and Softmax for multiclass classification and choose the method that provides the best performance for your specific use case.\n",
    "\n",
    "20. How do we interpret coefficients in Logistic Regression?\n",
    "\n",
    "Coefficients in Logistic Regression represent the change in the log odds of the dependent variable for a one-unit increase in the corresponding independent variable. To interpret the coefficients, we can use the following formula:\n",
    "\n",
    "Log odds = Coefficient * Independent Variable Value\n",
    "\n",
    "For example, if the coefficient of a predictor variable is 0.5, and the predictor variable has a value of 1, then the log odds of the dependent variable for that instance increases by 0.5. To convert the log odds back to a probability, we can use the following formula:\n",
    "\n",
    "Probability = 1 / (1 + e^-Log Odds)\n",
    "\n",
    "By interpreting the coefficients, we can gain insights into the relationship between the independent variables and the dependent variable, as well as the impact of each independent variable on the dependent variable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as  np \n",
    "import  pandas as pd \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import uniform\n",
    "from collections import Counter\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV ,StratifiedKFold, cross_val_score , RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, classification_report,roc_auc_score, roc_curve, auc ,  cohen_kappa_score\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic Regression, and prints the model accuracy\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "X = iris.data  \n",
    "y = (iris.target == 0).astype(int)  \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.  Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1') and print the model accuracy\n",
    "\n",
    "model = LogisticRegression(penalty='l1', solver='liblinear', C=1.0) \n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy with L1 Regularization: {accuracy:.4f}\")\n",
    "print(\"Feature Coefficients:\", model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Write a Python program to train Logistic Regression with L2 regularization (Ridge) using LogisticRegression(penalty='l2'). Print model accuracy and coefficients\n",
    "\n",
    "model = LogisticRegression(penalty='l2') \n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy with L2 Regularization: {accuracy:.4f}\")\n",
    "print(\"Feature Coefficients:\", model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet')\n",
    "\n",
    "model = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, C=1.0)  \n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Model Accuracy with Elastic Net Regularization: {accuracy:.4f}\")\n",
    "print(\"Feature Coefficients:\", model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr'C\n",
    "\n",
    "model = LogisticRegression(multi_class='ovr', solver='liblinear')  \n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy (Multiclass with OvR): {accuracy:.4f}\")\n",
    "print(\"Feature Coefficients (per class):\", model.coef_)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic Regression. Print the best parameters and accuracy\n",
    "\n",
    "model = LogisticRegression(solver='saga', multi_class='ovr', max_iter=500)  \n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],  \n",
    "    'penalty': ['l1', 'l2'] \n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "print(f\"Best Model Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. C Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the average accuracyC\n",
    "\n",
    "model = LogisticRegression(multi_class='ovr', solver='liblinear')  \n",
    "\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)  # 5-fold cross-validation\n",
    "\n",
    "scores = cross_val_score(model, X, y, cv=kf, scoring='accuracy')\n",
    "\n",
    "print(f\"Accuracy for each fold: {scores}\")\n",
    "print(f\"Average Accuracy: {np.mean(scores):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its accuracy.\n",
    "\n",
    "df = sns.load_dataset(\"titanic\")\n",
    "\n",
    "df = df.dropna()\n",
    "\n",
    "df['sex'] = df['sex'].map({'male': 0, 'female': 1})\n",
    "df[\"embarked\"]= df[\"embarked\"].map({'S':1, 'C':2, 'Q':3})\n",
    "df[\"class\"] = df[\"class\"].map({'Third':3, 'First':1, 'Second':2})\n",
    "\n",
    "X = df.drop([ 'who', 'adult_male', 'deck', 'embark_town','alive', 'alone'] , axis=1)\n",
    "y = df[\"survived\"] \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Model Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in Logistic Regression. Print the best parameters and accuracyM\n",
    " \n",
    "model = LogisticRegression(multi_class='ovr', max_iter=500)\n",
    "\n",
    "param_dist = {\n",
    "    'C': uniform(0.01, 10),  \n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear', 'saga'] \n",
    "}\n",
    "random_search = RandomizedSearchCV(model, param_dist, n_iter=10, cv=5, random_state=1 )\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "best_model = random_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Best Hyperparameters:\", random_search.best_params_)\n",
    "print(f\"Best Model Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy\n",
    "\n",
    "model = OneVsOneClassifier(LogisticRegression(solver='liblinear'))  \n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy (Multiclass with OvO): {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary classification\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(5, 4))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Class 0\", \"Class 1\"], yticklabels=[\"Class 0\", \"Class 1\"])\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(f\"Confusion Matrix (Accuracy: {accuracy:.4f})\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Write a Python program to train a Logistic Regression model and evaluate its performance using Precision, Recall, and F1-ScoreM\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13 Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to improve model performan\n",
    "\n",
    "df = sns.load_dataset(\"titanic\")\n",
    "\n",
    "print(\"Class distribution:\", Counter(df.iloc[:, -1]))\n",
    "weights = {0: 1, 1: 10}  \n",
    "\n",
    "model = LogisticRegression(class_weight='balanced', solver='liblinear')  \n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14. Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and evaluate performance\n",
    "\n",
    "# Load the Titanic dataset\n",
    "data_url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
    "df = pd.read_csv(data_url)\n",
    "\n",
    "# Select relevant features\n",
    "features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
    "df = df[features + ['Survived']]\n",
    "\n",
    "# Handle missing values\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "df[['Age', 'Embarked']] = imputer.fit_transform(df[['Age', 'Embarked']])\n",
    "\n",
    "# Convert categorical variables into numerical\n",
    "df['Sex'] = df['Sex'].map({'male': 0, 'female': 1})\n",
    "df = pd.get_dummies(df, columns=['Embarked'], drop_first=True)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X = df.drop(columns=['Survived'])\n",
    "y = df['Survived']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize numerical features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Print results\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15 Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression model. Evaluate its accuracy and compare results with and without scaling\n",
    "\n",
    "df = sns.load_dataset(\"titanic\")\n",
    "\n",
    "df = df[[\"pclass\", \"sex\", \"age\", \"fare\", \"survived\"]]\n",
    "\n",
    "df[\"age\"].fillna(df[\"age\"].median(), inplace=True)\n",
    "\n",
    "df[\"sex\"] = df[\"sex\"].map({\"male\": 1, \"female\": 0})\n",
    "\n",
    "X = df.drop(columns=[\"survived\"])  \n",
    "y = df[\"survived\"]  \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "model_no_scaling = LogisticRegression()\n",
    "model_no_scaling.fit(X_train, y_train)\n",
    "y_pred_no_scaling = model_no_scaling.predict(X_test)\n",
    "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "model_scaled = LogisticRegression()\n",
    "model_scaled.fit(X_train_scaled, y_train)\n",
    "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
    "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
    "\n",
    "print(f\"Accuracy WITHOUT Scaling: {accuracy_no_scaling:.4f}\")\n",
    "print(f\"Accuracy WITH Scaling: {accuracy_scaled:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16. Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score\n",
    "\n",
    "df = sns.load_dataset(\"titanic\")\n",
    "\n",
    "df = df[[\"pclass\", \"sex\", \"age\", \"fare\", \"survived\"]]\n",
    "\n",
    "df[\"age\"].fillna(df[\"age\"].median(), inplace=True)\n",
    "\n",
    "df[\"sex\"] = df[\"sex\"].map({\"male\": 1, \"female\": 0})\n",
    "\n",
    "X = df.drop(columns=[\"survived\"])  \n",
    "y = df[\"survived\"]  \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "\n",
    "y_prob = model.predict_proba(X_test_scaled)[:, 1]  \n",
    "\n",
    "\n",
    "roc_auc = roc_auc_score(y_test, y_prob)\n",
    "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color=\"blue\", label=f\"ROC Curve (AUC = {roc_auc:.4f})\")\n",
    "plt.plot([0, 1], [0, 1], color=\"red\", linestyle=\"--\") \n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Receiver Operating Characteristic (ROC) Curve\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 17 Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate accuracy\n",
    "\n",
    "model = LogisticRegression(C=0.5, random_state=42)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy with C=0.5: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18. Write a Python program to train Logistic Regression and identify important features based on model coefficients\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    \"Feature\": X.columns,\n",
    "    \"Coefficient\": model.coef_[0]  \n",
    "})\n",
    "\n",
    "feature_importance[\"Abs_Coefficient\"] = feature_importance[\"Coefficient\"].abs()\n",
    "feature_importance = feature_importance.sort_values(by=\"Abs_Coefficient\", ascending=False)\n",
    "\n",
    "print(feature_importance[[\"Feature\", \"Coefficient\"]])\n",
    "\n",
    "\n",
    "plt.barh(feature_importance[\"Feature\"], feature_importance[\"Coefficient\"], color=\"skyblue\")\n",
    "plt.xlabel(\"Coefficient Value\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title(\"Feature Importance in Logistic Regression\")\n",
    "plt.axvline(x=0, color=\"red\", linestyle=\"--\")  \n",
    "plt.gca().invert_yaxis()  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 19. Write a Python program to train Logistic Regression and evaluate its performance using Cohen‚Äôs Kappa Score\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "kappa_score = cohen_kappa_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Cohen‚Äôs Kappa Score: {kappa_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20. Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary classification\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_prob)\n",
    "\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall, precision, color=\"blue\", label=f\"PR Curve (AUC = {pr_auc:.4f})\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 21. Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare their accuracy\n",
    "\n",
    "solvers = [\"liblinear\", \"saga\", \"lbfgs\"]\n",
    "results = {}\n",
    "\n",
    "for solver in solvers:\n",
    "    model = LogisticRegression(solver=solver, random_state=42, max_iter=500)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "  \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    results[solver] = accuracy\n",
    "\n",
    "print(\"Logistic Regression Solver Comparison:\")\n",
    "for solver, acc in results.items():\n",
    "    print(f\"Solver: {solver}, Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 22. M Write a Python program to train Logistic Regression and evaluate its performance using Matthews Correlation Coefficient (MCC)\n",
    "\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Matthews Correlation Coefficient (MCC): {mcc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 23. Write a Python program to train Logistic Regression on both raw and standardized data. Compare their accuracy to see the impact of feature scalingM\n",
    "\n",
    "model_raw = LogisticRegression()\n",
    "model_raw.fit(X_train, y_train)\n",
    "y_pred_raw = model_raw.predict(X_test)\n",
    "accuracy_raw = accuracy_score(y_test, y_pred_raw)\n",
    "\n",
    "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
    "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
    "\n",
    "print(\"Logistic Regression Accuracy Comparison:\")\n",
    "print(f\"Without Scaling: {accuracy_raw:.4f}\")\n",
    "print(f\"With Scaling (Standardized Data): {accuracy_scaled:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 24. Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using cross-validation\n",
    "\n",
    "model = LogisticRegression(max_iter=500, solver=\"liblinear\")\n",
    "\n",
    "param_grid = {\"C\": np.logspace(-4, 4, 10)}  \n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring=\"accuracy\", n_jobs=-1)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "best_C = grid_search.best_params_[\"C\"]\n",
    "best_model = LogisticRegression(C=best_C, max_iter=500, solver=\"liblinear\")\n",
    "best_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Best C Value: {best_C}\")\n",
    "print(f\"Final Model Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 25. Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to make predictions.\n",
    "\n",
    "\n",
    "import joblib  # Import joblib for saving/loading models\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "joblib.dump(model, \"logistic_regression_model.pkl\")\n",
    "\n",
    "loaded_model = joblib.load(\"logistic_regression_model.pkl\")\n",
    "\n",
    "y_pred = loaded_model.predict(X_test_scaled)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Loaded Model Accuracy: {accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
