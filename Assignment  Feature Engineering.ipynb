{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What is a parameter?  \n",
    "Ans  \n",
    " Feature Engineering, the term parameter refers to a value or a setting that helps define how a feature is transformed, processed, or used in a model. \n",
    "Parameters influence how data is represented, scaled, encoded, or manipulated to improve the predictive power of machine learning algorithms.  \n",
    " \n",
    "  * Parameter in Feature Transformation:\n",
    "    In the context of feature engineering, parameters could refer to values that guide transformations or modifications to the raw data. \n",
    "\n",
    "  *  Parameter in Model Building:\n",
    "    Parameters are also used when selecting which features are fed into the model  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. What is correlation?   \n",
    "   What does negative correlation mean?  \n",
    "\n",
    "Correlation refers to a statistical relationship between two or more variables, indicating how one variable changes in relation to another. In simpler terms, correlation measures the degree to which two variables move together    \n",
    "\n",
    "Negative correlation refers to a relationship between two variables where, as one variable increases, the other decreases, and vice versa. In other words, when one variable goes up, the other tends to go down  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Define Machine Learning. What are the main components in Machine Learning?  \n",
    "\n",
    "* Data (Training and Testing)\n",
    "* Model (Mathematical structure)\n",
    "* Algorithm (Training method)\n",
    "* Features and Labels (Input and Output)\n",
    "* Training (Learning process)\n",
    "* Evaluation Metrics (Performance assessment)\n",
    "* Hyperparameters (Configuration settings)\n",
    "* Optimization (Improvement of the model's accuracy)  \n",
    "These components work together to create a machine learning system that can analyze data, learn from it, and make predictions or decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. How does loss value help in determining whether the model is good or not?  \n",
    "\n",
    "* Lower Loss = Better Model: A model with a lower loss value is typically better because it indicates that its predictions are closer to the true values.  \n",
    "* High Loss = Poor Model: A high loss value means the model is performing poorly, making significant errors in its predictions.  \n",
    "* Loss Value Behavior: Tracking how the loss value changes over time helps in identifying issues like overfitting, underfitting, or improper model training.  \n",
    "\n",
    "In conclusion, loss helps determine whether a model is good or not by providing an objective measure of prediction accuracy, guiding the optimization process, and helping identify potential problems such as overfitting or underfitting. A model with a consistently low loss value is generally considered a good model for the given task  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. What are continuous and categorical variables?\n",
    "\n",
    "* Continuous Variables:\n",
    "A continuous variable is a type of numerical variable that can take any value within a certain range or interval. These variables are measured and can represent a wide spectrum of possible values, including decimals or fractions\n",
    "\n",
    "    Examples of Continuous Variables:\n",
    "     * Height (e.g., 170.5 cm, 180.2 cm)\n",
    "     * Weight (e.g., 65.3 kg, 82.1 kg)\n",
    "     * Temperature (e.g., 22.5°C, 30.1°C)\n",
    "\n",
    "* Categorical Variables:  \n",
    "A categorical variable is a type of variable that represents categories or groups. These variables can take on one of a limited, fixed number of values. Categorical variables are often referred to as qualitative because they describe qualities or characteristics rather than quantities.   \n",
    "\n",
    "    Examples of Categorical Variables:\n",
    "     * Gender (e.g., \"Male\", \"Female\", \"Other\")\n",
    "     * Marital Status (e.g., \"Single\", \"Married\", \"Divorced\")\n",
    "     * Country (e.g., \"USA\", \"Canada\", \"India\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. How do we handle categorical variables in Machine Learning? What are the common techniques?  \n",
    "\n",
    "* Label Encoding: Suitable for ordinal variables.\n",
    "* One-Hot Encoding: Best for nominal variables, but can increase dimensionality.\n",
    "* Ordinal Encoding: A form of label encoding specifically for ordinal variables.\n",
    "* Target Encoding: Suitable for high-cardinality categorical variables, but can lead to overfitting.\n",
    "* Frequency/Count Encoding: Uses category frequency, suitable for high-cardinality features.\n",
    "* Binary Encoding: A compromise between label and one-hot encoding, good for high-cardinality features.\n",
    "* Hashing: Useful for very high-cardinality variables, but may result in hash collisions.\n",
    "* Embeddings: Deep learning method for handling high-cardinality variables effectively.  \n",
    "\n",
    "Choosing the appropriate method depends on the nature of the categorical variable (ordinal or nominal), the cardinality of the variable, and the machine learning model being used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. What do you mean by training and testing a dataset?  \n",
    "\n",
    "* Training Set:\n",
    "A subset of the dataset used to train the machine learning model. Typically, 70-80% of the data is used for training.  \n",
    "\n",
    "* Testing Set:\n",
    "\n",
    "A separate subset of the dataset used to evaluate the model's performance after it has been trained. Typically, 20-30% of the data is used for testing.\n",
    "* Validation Set:\n",
    "\n",
    "In addition to training and testing, a validation set is sometimes used (especially in complex models like deep learning). This set is used to fine-tune model hyperparameters during training. It helps in selecting the best model and prevents overfitting on the training data. In practice, this can be a separate set or achieved through techniques like cross-validation.\n",
    "* Overfitting:\n",
    "\n",
    "When a model learns the training data too well, including noise or irrelevant details, it performs well on the training set but poorly on the testing set. This is a sign that the model has memorized the training data rather than learning general patterns.\n",
    "* Underfitting:\n",
    "\n",
    "When a model is too simplistic and cannot capture the underlying patterns in the training data, leading to poor performance on both the training and testing sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. What is sklearn.preprocessing?\n",
    "\n",
    "sklearn.preprocessing is a module in the scikit-learn library (often referred to as sklearn), which provides a set of functions and classes for transforming and scaling data to improve the performance of machine learning algorithms. The preprocessing tools in this module help prepare your data by transforming it into a format suitable for training machine learning models. These transformations can include operations like scaling features, encoding categorical variables, or handling missing data  \n",
    "\n",
    "* Scaling techniques (e.g., StandardScaler, MinMaxScaler) to normalize feature ranges.\n",
    "* Encoding methods (e.g., LabelEncoder, OneHotEncoder) for converting categorical data into numeric format.\n",
    "* Imputation tools (e.g., SimpleImputer) for handling missing values.\n",
    "* Polynomial feature generation (e.g., PolynomialFeatures) for extending linear models.\n",
    "* Binarization techniques to convert continuous data into binary form.\n",
    "* Proper data preprocessing is essential to improve the performance, accuracy, and efficiency of machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. What is a Test set?  \n",
    "\n",
    "A test set is a subset of data used to evaluate the performance of a machine learning model after it has been trained. The test set is separate from the training data and is used to assess how well the model generalizes to unseen data. The main purpose of the test set is to provide an unbiased evaluation of a model's performance, helping you understand how the model will perform on real-world, unseen data.\n",
    "\n",
    "* Used for Model Evaluation: The test set is used only after the model has been trained. It provides an indication of how well the model can make predictions on new, unseen data.\n",
    "\n",
    "* Not Involved in Training: Unlike the training set, which is used to train the model, the test set is kept separate and is not used during training. This prevents data leakage, where the model might overfit to the training data and perform poorly on new data.\n",
    "\n",
    "* Unseen Data: The test set contains data that was not seen by the model during training, so it mimics real-world scenarios where the model will encounter new data after deployment.\n",
    "\n",
    "* Performance Metrics: After the model is tested on the test set, performance metrics such as accuracy, precision, recall, F1 score, mean squared error (MSE), etc., are calculated to assess the model's effectiveness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. How do we split data for model fitting (training and testing) in Python?  \n",
    "    How do you approach a Machine Learning problem?  \n",
    "\n",
    "In machine learning, it's crucial to split the dataset into two (or more) subsets: training data and test data. The training data is used to fit the model, while the test data is used to evaluate the model's performance on unseen data. This helps prevent overfitting and ensures that the model can generalize well.\n",
    "* Understand the problem.\n",
    "* Collect and preprocess data.\n",
    "* Split data into training and test sets.\n",
    "* Choose and train a model.\n",
    "* Evaluate the model.\n",
    "* Refine the model.\n",
    "* Deploy and monitor.\n",
    "This structured approach ensures that the model not only fits well to the data but also generalizes well to new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Why do we have to perform EDA before fitting a model to the data?\n",
    "\n",
    "Performing EDA before fitting a model is essential for gaining a deep understanding of your data, identifying potential issues, selecting relevant features, and making informed decisions about the modeling approach. It sets the foundation for building robust, high-performing machine learning models that generalize well to unseen data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. What is correlation?  \n",
    "\n",
    "* Correlation is a statistical measure that describes the strength and direction of a relationship between two or more variables. It helps to understand how changes in one variable are associated with changes in another variable. In simpler terms, correlation indicates whether and how two variables move together.\n",
    "\n",
    "* Correlation is a useful statistical tool for identifying relationships between variables. By analyzing correlation, you can gain insights into how variables are related and make more informed decisions in data analysis and modeling. However, it's crucial to remember that correlation alone does not imply causality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example DataFrame\n",
    "data = {'height': [150, 160, 170, 180, 190],\n",
    "        'weight': [50, 60, 70, 80, 90]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Calculate Pearson correlation\n",
    "correlation = df['height'].corr(df['weight'])\n",
    "print(f\"Correlation coefficient: {correlation}\")\n",
    "\n",
    "plt.scatter(df['height'], df['weight'])\n",
    "plt.title(' Correlation between hight and weight')\n",
    "plt.xlabel('height')\n",
    "plt.ylabel('weight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. What does negative correlation mean?  \n",
    "\n",
    "A negative correlation refers to a relationship between two variables in which one variable increases while the other decreases, or vice versa. In other words, when one variable moves in one direction (either up or down), the other variable moves in the opposite direction. This type of relationship is also known as an inverse relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "data = {'age': [5, 6, 7, 8, 9, 10],  # Age of children\n",
    "        'hours_sleep': [12, 11.5, 11, 10.5, 10, 9.5]}  # Hours of sleep\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "correlation = df['age'].corr(df['hours_sleep'])\n",
    "print(f\"Correlation coefficient: {correlation}\")\n",
    "\n",
    "# Plotting the data\n",
    "plt.scatter(df['age'], df['hours_sleep'])\n",
    "plt.title('Negative Correlation between Age and Hours of Sleep')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Hours of Sleep')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. How can you find correlation between variables in Python?  \n",
    "\n",
    "You can find the correlation between variables in Python using the corr() method from pandas. For example, if you have a DataFrame df, you can use df.corr() to calculate pairwise correlations between numerical columns. To delve deeper, you might also consider using seaborn or matplotlib to visualize the correlation matrix with a heatmap. What type of data are you working with?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15. What is causation? Explain difference between correlation and causation with an example\n",
    "\n",
    "Causation refers to a relationship where one variable directly causes an effect in another. In other words, a change in one variable directly leads to a change in another variable.  \n",
    "\n",
    "Correlation, on the other hand, refers to a statistical relationship between two variables, where they tend to move together, but one does not necessarily cause the other. Correlation only measures the strength and direction of the relationship between variables, but it doesn't imply that one causes the other.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16. What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
    "\n",
    "n machine learning and deep learning, an optimizer is an algorithm or method used to minimize (or maximize) a loss function by adjusting the model's parameters (weights and biases). The goal of an optimizer is to find the best parameters (model weights) that minimize the error between the model's predictions and the actual output (targets). Optimizers are essential in the training process of neural networks, as they enable the model to learn from data.\n",
    "\n",
    "* SGD: Simple, often used with momentum.\n",
    "* Momentum: Speeds up convergence by using past gradients.\n",
    "* Adagrad: Adapts learning rates based on past gradients, but may become too aggressive over time.\n",
    "* RMSprop: Similar to Adagrad but uses a moving average to stabilize updates.\n",
    "* Adam: Combines momentum and RMSprop, widely used due to its efficiency and effectiveness.\n",
    "* Adadelta: Addresses issues in Adagrad by limiting the accumulation of past gradients.\n",
    "\n",
    "Choosing the right optimizer depends on the problem, dataset, and model architecture. Adam is a popular choice for most deep learning tasks due to its adaptability and good performance across various tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "17. What is sklearn.linear_model ?\n",
    "\n",
    "sklearn.linear_model is a module within Scikit-learn, a popular machine learning library in Python. This module provides a variety of linear models that are used for supervised learning tasks, such as regression and classification. Linear models assume a linear relationship between the input features and the target variable. These models are fundamental and often serve as a baseline for more complex models.  \n",
    "\n",
    "* Fit models using gradient descent or closed-form solutions: Models like Ridge, Lasso, and Linear Regression are typically fit using either the closed-form solution (like the Normal Equation for linear regression) or through iterative methods like gradient descent.  \n",
    "\n",
    "* Regularization: Models like Ridge, Lasso, and ElasticNet add regularization terms to the loss function to control the complexity of the model, prevent overfitting, and handle multicollinearity.\n",
    "\n",
    "* Efficient implementation: Scikit-learn's linear models are highly optimized for performance and scalability, making them suitable for large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "18. What does model.fit() do? What arguments must be given?\n",
    "\n",
    "The model.fit() method in machine learning is used to train a model on the provided data. It adjusts the model's internal parameters (such as weights in neural networks or coefficients in linear models) based on the training data. The goal is to learn the underlying patterns in the data to make accurate predictions on new, unseen data.  \n",
    "\n",
    "* Training the Model:When you call model.fit(X_train, y_train), the model uses the input data (X_train) and corresponding target labels (y_train) to learn the relationship between the inputs and outputs. This involves optimizing the model's parameters (such as weights and biases in neural networks or coefficients in linear regression).\n",
    "\n",
    "* Optimization:The method works by minimizing (or maximizing) a loss function, which quantifies the difference between the model's predictions and the actual target values. During training, the model's parameters are adjusted iteratively using optimization algorithms like gradient descent (for most models) or closed-form solutions (for simpler models like linear regression).\n",
    "\n",
    "* Model Evaluation:Some models might also evaluate their performance during training (e.g., by calculating the loss on a validation set), but this depends on the model and whether it's configured to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "19. What does model.predict() do? What arguments must be given?\n",
    "\n",
    "The model.predict() method in machine learning is used to make predictions based on the trained model. After training a model using the fit() method, you can use predict() to generate predictions for new, unseen data. The predictions are based on the patterns the model learned during the training process.  \n",
    "\n",
    "* Required argument: X, which is the input data for prediction. It should have the same number of features as the training data.\n",
    "* Output: Predicted values or class labels depending on the problem type (regression or classification)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20. What are continuous and categorical variables? \n",
    "\n",
    "* Continuous Variables:\n",
    "A continuous variable is a variable that can take an infinite number of values within a given range. These values are typically measurements and can be broken down into finer increments. Continuous variables are usually quantitative, meaning they represent amounts or quantities that can be measured on a scale.  \n",
    "\n",
    "* Categorical Variables:\n",
    "A categorical variable is a variable that represents categories or distinct groups. These variables are qualitative, not quantitative. Categorical variables are typically used to represent data that can be sorted into specific groups or categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "21. What is feature scaling? How does it help in Machine Learning?\n",
    "\n",
    "standardize or normalize the range of independent variables or features of the data. It involves transforming the features so that they have similar scales or values, which is important because many machine learning algorithms perform better when the input features are on a comparable scale.  \n",
    "\n",
    "* Improves Algorithm Performance\n",
    "* Faster Convergence in Gradient-Based Algorithms\n",
    "* Improved Accuracy\n",
    "* Preventing Bias from Large Feature Values\n",
    "* Handling Outliers in Robust Scaling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "22. How do we perform scaling in Python?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n",
    "\n",
    "# Min-Max Scaling\n",
    "from  sklearn.preprocessing import MinMaxScaler \n",
    "scal = MinMaxScaler()\n",
    "\n",
    "scaled_data = scal.fit_transform(data)\n",
    "\n",
    "#  Standardization (Z-score Normalization)\n",
    "\n",
    "from  sklearn.preprocessing import StandardScaler\n",
    "scale = StandardScaler()\n",
    "\n",
    "z_score = scale.fit_transform(data)\n",
    "\n",
    "#  RobustScaler\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "scaled = RobustScaler()\n",
    "\n",
    "rob = scaled.fit_transform(data)\n",
    "\n",
    "# print the scale\n",
    "\n",
    "print(\"Scaled Data (Min-Max Scaling):\\n\", scaled_data)\n",
    "print()\n",
    "print(\"Scaled Data (Standardization):\\n\", z_score)\n",
    "print()\n",
    "print(\"Scaled Data (Robust Scaling):\\n\", rob)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "23. What is sklearn.preprocessing?\n",
    "\n",
    "sklearn.preprocessing is a module in the scikit-learn library that provides a set of functions and classes for preprocessing data before it is fed into machine learning algorithms. Preprocessing refers to the steps taken to prepare raw data for analysis and model building, ensuring that the data is in the right format and scale for the algorithm to perform effectively.  \n",
    "\n",
    "* Scaling and Normalization: Methods like StandardScaler, MinMaxScaler, and RobustScaler standardize or normalize data.\n",
    "* Encoding Categorical Data: LabelEncoder and OneHotEncoder convert categorical data into numeric form.\n",
    "* Imputation: SimpleImputer handles missing values.\n",
    "* Polynomial Features: PolynomialFeatures helps create non-linear relationships by adding polynomial features.\n",
    "* Binarization: Binarizer turns numerical data into binary form.\n",
    "* Power Transformation: PowerTransformer makes data more Gaussian-like for better model performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "24. How do we split data for model fitting (training and testing) in Python?\n",
    "\n",
    "In Python, particularly using the scikit-learn library, you can split your dataset into training and testing sets using the train_test_split() function from the sklearn.model_selection module. This is an essential step in building machine learning models to ensure that your model is evaluated on unseen data (the test set) and not just trained on it (the training set).\n",
    "\n",
    "* train_test_split() is the go-to function for splitting data into training and testing sets in Python using scikit-learn.\n",
    "* The most important parameters are X (features), y (target), test_size (size of the test set), and random_state (for reproducibility)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features:\n",
      "    Feature1  Feature2\n",
      "4         5         1\n",
      "2         3         3\n",
      "0         1         5\n",
      "3         4         2\n",
      "Testing Features:\n",
      "    Feature1  Feature2\n",
      "1         2         4\n",
      "Training Labels:\n",
      " 4    0\n",
      "2    0\n",
      "0    0\n",
      "3    1\n",
      "Name: Target, dtype: int64\n",
      "Testing Labels:\n",
      " 1    1\n",
      "Name: Target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Feature1': [1, 2, 3, 4, 5],\n",
    "    'Feature2': [5, 4, 3, 2, 1],\n",
    "    'Target': [0, 1, 0, 1, 0]\n",
    "})\n",
    "\n",
    "\n",
    "X = df[['Feature1', 'Feature2']]  # Features\n",
    "y = df['Target']  # Target variable\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Output the results\n",
    "print(\"Training Features:\\n\", X_train)\n",
    "print(\"Testing Features:\\n\", X_test)\n",
    "print(\"Training Labels:\\n\", y_train)\n",
    "print(\"Testing Labels:\\n\", y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "25. Explain data encoding?\n",
    "\n",
    "ata encoding refers to the process of converting categorical data (non-numeric) into a numeric format so that machine learning models can work with it. Machine learning algorithms typically require numerical inputs to perform computations, so encoding is an essential preprocessing step when dealing with categorical variables.   \n",
    "\n",
    "Types of Data Encoding\n",
    "\n",
    "1. Label Encoding : \n",
    "Label encoding involves converting each unique category value into a unique integer. This is useful for ordinal variables, where there is an inherent order in the categories (e.g., \"low\", \"medium\", \"high\").\n",
    "\n",
    "2. One-Hot Encoding :\n",
    "One-hot encoding is used for nominal categorical variables where there is no ordinal relationship between categories. It creates a new binary column for each category in the original feature, with values of either 0 or 1 indicating whether a sample belongs to that category.\n",
    "\n",
    "3. Ordinal Encoding\n",
    "Ordinal encoding is a method of converting categorical data into numerical format, where the categories have an inherent order or ranking. Unlike one-hot encoding, which creates a binary vector for each category without any notion of order, ordinal encoding preserves the rank or order between categories by assigning integer values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Encoder [2 1 0 1 2]\n",
      "One-Hot Encoding\n",
      "   (0, 2)\t1.0\n",
      "  (1, 1)\t1.0\n",
      "  (2, 0)\t1.0\n",
      "  (3, 2)\t1.0\n",
      "  Satisfaction  Satisfaction_encoded\n",
      "0          Low                     0\n",
      "1       Medium                     1\n",
      "2         High                     2\n",
      "3       Medium                     1\n",
      "4          Low                     0\n"
     ]
    }
   ],
   "source": [
    "# Label Encoder\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Sample categorical data\n",
    "data = ['small', 'medium', 'large', 'medium', 'small']\n",
    "\n",
    "# Initialize the label encoder\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# Encode the categorical data\n",
    "encoded_data = encoder.fit_transform(data)\n",
    "\n",
    "print(\"Label Encoder\",encoded_data) \n",
    "\n",
    "# One-Hot Encoding\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "data = {'Color': ['Red', 'Green', 'Blue', 'Red']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "encoder_one = encoder.fit_transform(df)\n",
    "print(\"One-Hot Encoding\\n\", encoder_one)\n",
    "\n",
    "# ordinal Encoding \n",
    "\n",
    "data = {'Satisfaction': ['Low', 'Medium', 'High', 'Medium', 'Low']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "mapping = {'Low': 0, 'Medium': 1, 'High': 2}\n",
    "\n",
    "df['Satisfaction_encoded'] = df['Satisfaction'].map(mapping)\n",
    "\n",
    "print(df)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
