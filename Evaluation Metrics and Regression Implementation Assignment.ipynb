{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theoretical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What does R-squared represent in a regression model   \n",
    "Ans  \n",
    "   R-squared represents the proportion of the variance in the dependent variable that is explained by the independent variables in the regression model. It ranges from 0 to 1, where 1 indicates a perfect fit of the model to the data.  \n",
    "\n",
    "2. What are the assumptions of linear regression    \n",
    "Ans     \n",
    "   Linear regression assumes that the relationship between the dependent and independent variables is linear, that there is no multicollinearity, that the error terms are normally distributed, and that the variance of the error terms is constant across all levels of the independent variables.  \n",
    "\n",
    "3. What is the difference between R-squared and Adjusted R-squared      \n",
    "Ans       \n",
    "   R-squared calculates the proportion of the variance in the dependent variable that is explained by the independent variables, while Adjusted R-squared adjusts for the number of independent variables in the model to account for the impact of adding new variables. Adjusted R-squared is generally higher than R-squared, as it penalizes the model for including irrelevant variables.  \n",
    "\n",
    "4. Why do we use Mean Squared Error (MSE)   \n",
    "Ans     \n",
    "   MSE measures the average squared difference between the predicted and actual values in the dependent variable. It is sensitive to outliers and can be used to compare different models.   \n",
    "\n",
    "\n",
    "5. What does an Adjusted R-squared value of 0.85 indicate   \n",
    "Ans     \n",
    "   An Adjusted R-squared value of 0.85 indicates that the model explains 85% of the variance in the dependent variable. This value is generally higher than the corresponding R-squared value, as it adjusts for the number of independent variables in the model.\n",
    "\n",
    "6. How do we check for normality of residuals in linear regression   \n",
    "Ans\n",
    "  To check for normality of residuals in linear regression, we can use a Q-Q plot, histogram, or statistical tests. A Q-Q plot shows the quantiles of the residuals against the theoretical quantiles of a standard normal distribution. If the residuals are normally distributed, the points should fall on a straight line. If the residuals are not normally distributed, they may not fall on a straight line and may not follow the standard normal pattern.  \n",
    "\n",
    "   A histogram can also be used to check for normality of residuals. If the residuals are normally distributed, the histogram should resemble a bell-shaped curve. If the residuals are not normally distributed, they may not resemble a bell-shaped curve and may not follow the standard normal pattern.    \n",
    "\n",
    "   Statistical tests such as Shapiro-Wilk, Kolmogorov-Smirnov, or the Durbin-Watson test can also be used to check for normality of residuals. These tests provide a p-value, which indicates the probability of observing the residuals under the assumption of normality. If the p-value is significant (e.g., less than 0.05), it suggests that the residuals are not normally distributed. \n",
    "   \n",
    "   Note: It is important to remember that normality of residuals is not a requirement for linear regression, but it is a good practice to check for normality of residuals to ensure that the model's assumptions are met.  \n",
    "   \n",
    "\n",
    "7. What is multicollinearity, and how does it impact regression   \n",
    "Ans  \n",
    "   Multicollinearity occurs when two or more independent variables in a regression model are highly correlated, causing the estimates of the independent variables to be unstable and difficult to interpret. This can lead to overfitting, where the model performs well on the training data but poorly on new data.  \n",
    "    To mitigate the impact of multicollinearity, we can use techniques such as: \n",
    "    - Removing one or more of the correlated variables  \n",
    "    - Using regularization techniques such as Lasso or Ridge regression \n",
    "    - Using feature selection techniques such as stepwise regression or backward elimination    \n",
    "    - Using dimensionality reduction techniques such as Principal Component Analysis (PCA) or Factor Analysis   \n",
    "    - Using statistical techniques such as variance inflation factor (VIF) or condition number  \n",
    "    - Using advanced machine learning algorithms that can handle multicollinearity, such as decision trees or neural networks   \n",
    "\n",
    "8. What is Mean Absolute Error (MAE)  \n",
    "Ans   \n",
    "   Mean Absolute Error (MAE) measures the average absolute difference between the predicted and actual values in the dependent variable. It is a more interpretable measure of the model's performance, as it measures the average absolute error in the predicted values.  \n",
    "\n",
    "9. What are the benefits of using an ML pipeline  \n",
    "Ans  \n",
    "   An ML pipeline is a sequence of data preprocessing steps and machine learning algorithms that can be used to automate the process of building and evaluating machine learning models. By using an ML pipeline, you can streamline the process of preprocessing data, selecting features, training models, and evaluating their performance. This can make the development and evaluation of machine learning models more efficient and easier to manage. \n",
    "   * Streamlined preprocessing: ML pipelines can automate the preprocessing steps, such as data cleaning, normalization, feature selection, and splitting the data into training and testing sets. This makes the development and evaluation of machine learning models more efficient and easier to manage.   \n",
    "   * Streamlined model training and evaluation: ML pipelines can automate the training and evaluation of machine learning models, such as logistic regression, decision trees, random forests, or neural networks. This makes the development and evaluation of machine learning models more efficient and easier to manage.   \n",
    "   * Easy to manage and share: ML pipelines can be easily saved and shared, making it easier to reproduce and collaborate on machine learning models. This can make the development and evaluation of machine learning models more efficient and easier to manage.  \n",
    "   * Improved model performance: ML pipelines can help improve the performance of machine learning models by automatically optimizing hyperparameters, selecting the best preprocessing steps, and preprocessing the data in a consistent and efficient manner. This can make the development and evaluation of machine learning models more efficient and easier to manage.  \n",
    "\n",
    "10. Why is RMSE considered more interpretable than MSE  \n",
    "Ans   \n",
    "    RMSE is more interpretable than MSE because it measures the average squared difference between the predicted and actual values in the dependent variable, which is in the same units as the dependent variable. This makes it easier to understand the model's performance and the impact of different variables on the dependent variable. On the other hand, MSE measures the average squared difference between the predicted and actual values in the dependent variable, which is in the square of the units of the dependent variable. This makes it harder to understand the model's performance and the impact of different variables on the dependent variable.  \n",
    "   \n",
    "    Note: RMSE is often used in machine learning to evaluate the performance of regression models, as it is more interpretable and easier to understand. However, it is important to remember that RMSE does not have the same meaning as R-squared or Adjusted R-squared in linear regression. \n",
    "\n",
    "11. What is pickling in Python, and how is it useful in ML  \n",
    "Ans  \n",
    "    Pickling in Python is a technique for serializing and deserializing objects. It allows you to save an object in a file and load it later, preserving its state and data. This can be useful in machine learning, as it allows you to save and load machine learning models, preprocessing steps, and other data that you need to reuse or share.    \n",
    "    Pickling can be useful in machine learning by saving the following: \n",
    "    - Machine learning models   \n",
    "    - Preprocessing steps   \n",
    "    - Hyperparameters   \n",
    "    - Other data that you need to reuse or share    \n",
    "    Pickling can also be useful in machine learning by loading the saved data and reusing it to train and evaluate machine learning models. \n",
    "    Note: Pickling is not a secure method for saving and loading machine learning models. It is important to use secure methods for saving and loading machine learning models, such as using encryption or secure file storage.    \n",
    "\n",
    "12. What does a high R-squared value mean   \n",
    "Ans     \n",
    "    A high R-squared value in linear regression indicates that the model explains a large proportion of the variance in the dependent variable. This means that the model's predictions are close to the actual values, and the model has a good fit to the data. A high R-squared value is generally a good indicator of the model's performance. However, it is important to remember that R-squared does not provide a measure of the model's ability to make accurate predictions, as it is calculated based on\n",
    "    the correlation between the independent variables and the dependent variable, not the independent variables themselves. \n",
    "\n",
    "    Note: A high R-squared value does not necessarily mean that the model is a good fit for the data. It is important to evaluate the model's performance, such as using statistical tests or cross-validation, to determine if the model is a good fit for the data.\n",
    "\n",
    "13. What happens if linear regression assumptions are violated      \n",
    "Ans     \n",
    "    Linear regression assumes that the independent variables are linearly related to the dependent variable, and that there is no multicollinearity between the independent variables. Violating these assumptions can lead to unreliable and inaccurate predictions.       \n",
    "\n",
    "    Note: Violating linear regression assumptions can lead to inaccurate predictions and poor model performance. It is important to evaluate the model's performance, such as using statistical tests or cross-validation, to determine if the model is a good fit for the data.    \n",
    "\n",
    "14. How can we address multicollinearity in regression      \n",
    "Ans     \n",
    "    To address multicollinearity in regression, we can use techniques such as:  \n",
    "    - Removing one or more of the correlated variables  \n",
    "    - Using regularization techniques such as Lasso or Ridge regression     \n",
    "    - Using feature selection techniques such as stepwise regression or backward elimination    \n",
    "    - Using dimensionality reduction techniques such as Principal Component Analysis (PCA) or Factor Analysis   \n",
    "    - Using statistical techniques such as variance inflation factor (VIF) or condition number  \n",
    "    - Using advanced machine learning algorithms that can handle multicollinearity, such as decision trees or neural networks   \n",
    "    Note: Addressing multicollinearity in regression can lead to improved model performance and reduced the risk of overfitting. It is important to evaluate the model's performance, such as using statistical tests or cross-validation, to determine if the model is a good fit for the data.    \n",
    "\n",
    "15. Why do we use pipelines in machine learning     \n",
    "Ans     \n",
    "    Pipelines in machine learning are a sequence of data preprocessing steps and machine learning algorithms that can be used to automate the process of building and evaluating machine learning models. By using pipelines, you can streamline the process of preprocessing data, selecting features, training models, and evaluating their performance. This can make the development and evaluation of machine learning models more efficient and easier to manage. \n",
    "\n",
    "    Note: Pipelines in machine learning can be useful in various applications, such as data preprocessing, feature selection, and model training. They can also be useful for sharing and reproducing machine learning models, making it easier to collaborate on machine learning projects.    \n",
    "\n",
    "16. How is Adjusted R-squared calculated    \n",
    "Ans     \n",
    "    Adjusted R-squared is a statistical measure that provides an adjusted estimate of the model's performance. It is calculated using the following formula:  \n",
    "    Adjusted R-squared = (1 - (1 - R-squared) * (n - 1) / (n - p - 1)) * 100  \n",
    "    where:\n",
    "    - R-squared is the coefficient of determination (or R-squared) in linear regression \n",
    "    - n is the number of observations   \n",
    "    - p is the number of independent variables (excluding the intercept)    \n",
    "    Adjusted R-squared is a statistic that penalizes the model for including irrelevant predictors and helps to determine the model's performance in a more reliable and accurate way. Adjusted R-squared is generally a better indicator of the model's performance than R-squared, as it takes into account the number of predictors in the model.    \n",
    "\n",
    "    Note: Adjusted R-squared is a useful measure in linear regression, as it helps to determine the model's performance in a more reliable and accurate way. It is important to remember that Adjusted R-squared does not provide a measure of the model's ability to make accurate predictions, as it is calculated based on the correlation between the independent variables and the dependent variable, not the independent variables themselves.   \n",
    "\n",
    "17. Why is MSE sensitive to outliers    \n",
    "Ans     \n",
    "    Mean Squared Error (MSE) is a statistical measure that measures the average squared difference between the predicted and actual values in the dependent variable. It is sensitive to outliers because outliers can significantly impact the MSE value. Outliers can cause the model to have a high bias, leading to inaccurate predictions and poor model performance.  \n",
    "\n",
    "    Note: MSE is a useful measure in linear regression, as it helps to determine the model's performance in a more reliable and accurate way. It is important to remember that MSE does not provide a measure of the model's ability to make accurate predictions, as it is calculated based on the correlation between the independent variables and the dependent variable, not the independent variables themselves.  \n",
    "\n",
    "18. What is the role of homoscedasticity in linear regression       \n",
    "Ans     \n",
    "    Homoscedasticity in linear regression refers to the assumption that the variance of the error term (the difference between the predicted and actual values) is constant across the range of the independent variables. In other words, the residuals have a constant variance. Homoscedasticity is a desirable property in linear regression, as it helps to improve the model's performance and make the predictions more accurate.\n",
    "\n",
    "    Note: Homoscedasticity is a critical assumption in linear regression\n",
    "\n",
    "19. What is Root Mean Squared Error (RMSE)      \n",
    "Ans     \n",
    "    Root Mean Squared Error (RMSE) is a statistical measure that measures the average squared difference between the predicted and actual values in the dependent variable. It is calculated using the following formula:   \n",
    "    RMSE = sqrt(MSE)    \n",
    "    where MSE is the Mean Squared Error \n",
    "    RMSE is sensitive to outliers because outliers can significantly impact the RMSE value. Outliers can cause the model to have a high bias, leading to inaccurate predictions and poor model performance.     \n",
    "    Note: RMSE is often used in machine learning to evaluate the performance of regression models, as it is more interpretable and easier to understand. However, it is important to remember that RMSE does not have the same meaning as R-squared or Adjusted R-squared in linear regression.     \n",
    "\n",
    "20. Why is pickling considered risky  \n",
    "Ans  \n",
    "    Pickling, while a convenient method for saving and loading objects in Python, can be considered risky due to the following reasons:\n",
    "    - Pickling can be easily exploited by malicious actors. They can create a pickle file that contains a malicious code or object, and when the pickle file is loaded, the malicious code or object will be executed.  \n",
    "    - Pickling can be easily de-serialized by unauthorized users. They can create a pickle file that contains a malicious code or object, and when the pickle file is loaded, the malicious code or object will be executed.  \n",
    "    - Pickling can be easily deserialized by attackers who have access to the pickle file. They can create a pickle file that contains a malicious code or object, and when the pickle file is loaded, the malicious code or object will be executed.  \n",
    "\n",
    "    Note: Pickling is a convenient and simple way to save and load objects in Python, but it is not a secure method for saving and loading ML models. It is important to use secure methods for saving and loading ML models, such as using encrypted files or secure key storage mechanisms.  \n",
    "\n",
    "21. What alternatives exist to pickling for saving ML models    \n",
    "Ans  \n",
    "    Alternatives to pickling for saving ML models include:\n",
    "    - Using a database: Storing ML models in a database can provide a secure and scalable solution for saving and loading ML models. Databases are designed to handle large amounts of data and provide efficient storage and retrieval.    \n",
    "    - Using a file system: Storing ML models in a file system can provide a secure and scalable solution for saving and loading ML models. File systems are designed to handle large amounts of data and provide efficient storage and retrieval.   \n",
    "    - Using a cloud storage service: Storing ML models in a cloud storage service can provide a secure and scalable solution for saving and loading ML models. Cloud storage services are designed to handle large amounts of data and provide efficient storage and retrieval. \n",
    "    - Using a secure key storage mechanism: Storing ML models in a secure key storage mechanism can provide a secure and scalable solution for saving and loading ML models. Secure key storage mechanisms are designed to protect sensitive data and provide a secure way to store and access ML models.   \n",
    "\n",
    "    Note: Using alternatives to pickling for saving ML models, such as using a database or cloud storage service, can provide a secure and scalable solution for saving and loading ML models. It is important to choose an alternative that meets your specific requirements and security needs.\n",
    "\n",
    "\n",
    "22.  What is heteroscedasticity, and why is it a problem    \n",
    "Ans     \n",
    "    Heteroscedasticity in linear regression refers to the assumption that the variance of the error term (the difference between the predicted and actual values) is not constant across the range of the independent variables. In other words, the residuals do not have a constant variance. Heteroscedasticity is a problem in linear regression because it can lead to inaccurate predictions and poor model performance.      \n",
    "\n",
    "    Note: Heteroscedasticity is a critical assumption in linear regression, and it is a problem in linear regression. It can be addressed by using transformations, such as taking the square root or logarithm, to make the variance of the error term constant across the range of the independent variables.  \n",
    "\n",
    "23. How does adding irrelevant predictors affect R-squared and Adjusted R-squared  \n",
    "Ans  \n",
    "    Adding irrelevant predictors to a linear regression model can significantly decrease the value of R-squared and Adjusted R-squared. R-squared and Adjusted R-squared are statistical measures that provide an estimate of the model's performance. By adding irrelevant predictors, you are essentially adding noise to the data, which can decrease the value of R-squared and Adjusted R-squared. This can make it difficult to determine the model's performance and make accurate predictions.  \n",
    "    \n",
    "    Note: Adding irrelevant predictors to a linear regression model can significantly decrease the value of R-squared and Adjusted R-squared, and it can make it difficult to determine the model's performance and make accurate predictions. It is important to carefully select and remove irrelevant predictors from the model to improve its performance and make accurate predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.pipeline import Pipeline   \n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error ,r2_score   \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Write a Python script that calculates the Mean Squared Error (MSE) and Mean Absolute Error (MAE) for a multiple linear regression model using Seaborn's \"diamonds\" datasetK\n",
    "\n",
    "diamonds = sns.load_dataset('diamonds')\n",
    "\n",
    "X = diamonds[['carat',   'depth', 'table']]\n",
    "y = diamonds['price']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)   \n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(f\"MSE: {mse}\")\n",
    "print(f\"MAE: {mae}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Write a Python script to calculate and print Mean Squared Error (MSE), Mean Absolute Error (MAE), and Root Mean Squared Error (RMSE) for a linear regression modelK\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "X = np.random.rand(1000, 1)\n",
    "y = 2 * X + np.random.randn(1000, 1)    \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Write a Python script to check if the assumptions of linear regression are met. Use a scatter plot to check linearity, residuals plot for homoscedasticity, and correlation matrix for multicollinearity  \n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "X, y = make_regression(n_samples=1000, n_features=1, noise=10, random_state=42)\n",
    "\n",
    "df = pd.DataFrame(X, columns=['feature'])\n",
    "df['target'] = y\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "sns.scatterplot(x='feature', y='target', data=df)\n",
    "\n",
    "residuals = y_test - y_pred\n",
    "\n",
    "sns.scatterplot(x=y_pred, y=residuals)\n",
    "\n",
    "correlation_matrix = df.corr()\n",
    "\n",
    "sns.heatmap(correlation_matrix, annot=True)\n",
    "\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Create a machine learning pipeline that standardizes the features, fits a linear regression model, and evaluates the model’s R-squared scor \n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "X, y = make_regression(n_samples=1000, n_features=1, noise=10, random_state=42)\n",
    "\n",
    "df = pd.DataFrame(X, columns=['feature'])\n",
    "\n",
    "df['target'] = y\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "pipeline = Pipeline([\n",
    "\n",
    "    ('standardization', StandardScaler()),\n",
    "\n",
    "    ('regression', model)\n",
    "\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "scores = cross_val_score(pipeline, X, y, cv=5, scoring='r2')\n",
    "\n",
    "print(\"Cross-Validation R-squared Scores:\", scores)\n",
    "\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Test Set R-squared Score:\", r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Implement a simple linear regression model on a dataset and print the model's coefficients, intercept, and R-squared score.\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "X, y = make_regression(n_samples=1000, n_features=1, noise=10, random_state=42)\n",
    "\n",
    "df = pd.DataFrame(X, columns=['feature'])\n",
    "\n",
    "df['target'] = y\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "coefficients = model.coef_\n",
    "\n",
    "intercept = model.intercept_\n",
    "\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Coefficients:\",model.coef_ )\n",
    "\n",
    "print(\"Intercept:\",  model.intercept_)\n",
    "\n",
    "print(\"R-squared Score:\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Write a Python script that analyzes the relationship between total bill and tip in the 'tips' dataset using simple linear regression and visualizes the results.\n",
    "\n",
    "tips = sns.load_dataset('tips')\n",
    "\n",
    "sns.scatterplot(x='total_bill', y='tip', data=tips)\n",
    "plt.title('Total Bill vs Tip')\n",
    "plt.show()\n",
    "\n",
    "# Perform linear regression\n",
    "\n",
    "X = tips[['total_bill']]\n",
    "y = tips['tip']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Print the coefficient and intercept\n",
    "\n",
    "coefficient = model.coef_[0]\n",
    "intercept = model.intercept_\n",
    "\n",
    "print(\"Coefficient:\", coefficient)\n",
    "print(\"Intercept:\", intercept)\n",
    "\n",
    "# Visualize the regression line\n",
    "\n",
    "plt.scatter(X, y)\n",
    "plt.plot(X, model.predict(X), color='red')\n",
    "plt.title('Total Bill vs Tip (with Regression Line)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Write a Python script that fits a linear regression model to a synthetic dataset with one feature. Use the model to predict new values and plot the data points along with the regression line.\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "X = np.random.rand(1000)\n",
    "y = 2 * X + np.random.randn(1000)\n",
    "\n",
    "df = pd.DataFrame({'feature': X, 'target': y})\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.reshape(-1, 1), y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train) \n",
    "\n",
    "y_pred = model.predict(X_test.reshape(-1, 1))\n",
    "plt.scatter(X_test, y_test)\n",
    "plt.plot(X_test, y_pred, color='red')\n",
    "plt.title('Synthetic Dataset (Linear Regression)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Write a Python script that pickles a trained linear regression model and saves it to a file.  \n",
    "\n",
    "import pickle\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "with open('linear_regression_model.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "    print(\"Linear regression model saved to 'linear_regression_model.pkl'\")\n",
    "    f.close()\n",
    "    # Load the model\n",
    "    with open('linear_regression_model.pkl', 'rb') as f:\n",
    "        loaded_model = pickle.load(f)\n",
    "        print(\"Linear regression model loaded from 'linear_regression_model.pkl'\")\n",
    "        f.close()\n",
    "        # Make predictions using the loaded model\n",
    "        y_pred = loaded_model.predict(X_test.reshape(-1, 1))\n",
    "        print(\"Predictions:\", y_pred)\n",
    "        # Evaluate the loaded model\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        print(\"Mean Squared Error (MSE):\", mse)\n",
    "        print(\"R-squared Score:\", r2)\n",
    "        f.close()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Write a Python script that fits a polynomial regression model (degree 2) to a dataset and plots the regression curve.\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "X = np.linspace(0, 10, 100)\n",
    "y = 2 * X**2 + 3 * X + np.random.randn(100)\n",
    "\n",
    "df = pd.DataFrame({'feature': X, 'target': y})\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.reshape(-1, 1), y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = make_pipeline(PolynomialFeatures(degree=2), LinearRegression())\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test.reshape(-1, 1))\n",
    "\n",
    "plt.scatter(X_test, y_test)\n",
    "plt.plot(X_test, y_pred, color='red')\n",
    "plt.title('Synthetic Dataset (Polynomial Regression with Degree 2)')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Generate synthetic data for simple linear regression (use random values for X and y) and fit a linear regression model to the data. Print the model's coefficient and intercept.\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "X = np.random.rand(1000)\n",
    "y = 2 * X + np.random.randn(1000)\n",
    "\n",
    "df = pd.DataFrame({'feature': X, 'target': y})\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.reshape(-1, 1), y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "coefficient = model.coef_[0]\n",
    "intercept = model.intercept_\n",
    "\n",
    "print(\"Coefficient:\", coefficient)\n",
    "print(\"Intercept:\", intercept)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Write a Python script that fits polynomial regression models of different degrees to a synthetic dataset and compares their performance.  \n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "X = np.linspace(0, 10, 100)\n",
    "y = 2 * X**2 + 3 * X + np.random.randn(100)\n",
    "\n",
    "df = pd.DataFrame({'feature': X, 'target': y})\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.reshape(-1, 1), y, test_size=0.2, random_state=42)\n",
    "\n",
    "degrees = [1, 2, 3, 4, 5]\n",
    "\n",
    "mse_scores = []\n",
    "\n",
    "r2_scores = []\n",
    "\n",
    "for degree in degrees:\n",
    "    model = make_pipeline(PolynomialFeatures(degree=degree), LinearRegression())\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test.reshape(-1, 1))\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mse_scores.append(mse)\n",
    "    r2_scores.append(r2)\n",
    "    print(f\"Degree {degree}: MSE={mse:.2f}, R-squared Score={r2:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Write a Python script that fits a simple linear regression model with two features and prints the model's coefficients, intercept, and R-squared score.\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "X = np.random.rand(1000, 2)\n",
    "y = 2 * X[:, 0] + 3 * X[:, 1] + np.random.randn(1000)\n",
    "\n",
    "df = pd.DataFrame(np.c_[X, y], columns=['feature1', 'feature2', 'target'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "coefficients = model.coef_\n",
    "\n",
    "intercept = model.intercept_\n",
    "\n",
    "r2 = r2_score(y_test, model.predict(X_test))\n",
    "\n",
    "print(\"Coefficients:\", coefficients)\n",
    "\n",
    "print(\"Intercept:\", intercept)\n",
    "\n",
    "print(\"R-squared Score:\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. Write a Python script that generates synthetic data, fits a linear regression model, and visualizes the regression line along with the data points.\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "X = np.linspace(0, 10, 100)\n",
    "y = 2 * X + np.random.randn(100)\n",
    "\n",
    "df = pd.DataFrame({'feature': X, 'target': y})\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.reshape(-1, 1), y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test.reshape(-1, 1))\n",
    "\n",
    "plt.scatter(X_test, y_test)\n",
    "plt.plot(X_test, y_pred, color='red')\n",
    "plt.title('Synthetic Dataset (Linear Regression)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14. Write a Python script that uses the Variance Inflation Factor (VIF) to check for multicollinearity in a dataset with multiple features.\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "data = fetch_california_housing()\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "\n",
    "df = pd.DataFrame(data.data , columns= data.feature_names)\n",
    "\n",
    "# Remove any missing values\n",
    "\n",
    "df = df.dropna()\n",
    "\n",
    "# Calculate VIF\n",
    "\n",
    "vif = pd.DataFrame()\n",
    "\n",
    "vif['features'] = df.columns\n",
    "vif['VIF'] = [variance_inflation_factor(df.values, i) for i in range(len(df.columns))]\n",
    "\n",
    "print(vif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15. Write a Python script that generates synthetic data for a polynomial relationship (degree 4), fits a polynomial regression model, and plots the regression curve.\n",
    "\n",
    "\n",
    "# Generate synthetic data\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "X = np.linspace(0, 10, 100)\n",
    "\n",
    "y = 2 * X**4 + 3 * X**3 - 4 * X**2 + 5 * X + np.random.randn(100)\n",
    "\n",
    "df = pd.DataFrame({'feature': X, 'target': y})\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.reshape(-1, 1), y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit a polynomial regression model\n",
    "\n",
    "poly_features = PolynomialFeatures(degree=4)\n",
    "\n",
    "X_train_poly = poly_features.fit_transform(X_train)\n",
    "X_test_poly = poly_features.transform(X_test)\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "model.fit(X_train_poly, y_train)\n",
    "\n",
    "# Make predictions\n",
    "\n",
    "y_pred = model.predict(X_test_poly)\n",
    "\n",
    "# Plot the data points and regression curve\n",
    "\n",
    "plt.scatter(X_test, y_test)\n",
    "plt.plot(X_test, y_pred, color='red')\n",
    "plt.title('Synthetic Dataset (Polynomial Regression)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.pipeline import make_pipeline  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16. Write a Python script that creates a machine learning pipeline with data standardization and a multiple linear regression model, and prints the R-squared score.\n",
    "\n",
    "from sklearn.pipeline import make_pipeline  \n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "data = fetch_california_housing()\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "\n",
    "df = pd.DataFrame(data.data , columns= data.feature_names)\n",
    "\n",
    "df['target'] = data.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('target', axis=1), df['target'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a machine learning pipeline\n",
    "\n",
    "pipeline = make_pipeline(StandardScaler(), LinearRegression())\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Print the R-squared score\n",
    "\n",
    "print(\"R-squared Score:\", pipeline.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 17. Write a Python script that performs polynomial regression (degree 3) on a synthetic dataset and plots the regression curve.\n",
    "\n",
    "# Generate synthetic data\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "X = np.linspace(0, 10, 100)\n",
    "\n",
    "y = 2 * X**3 + 3 * X**2 - 4 * X + 5 + np.random.randn(100)\n",
    "\n",
    "df = pd.DataFrame({'feature': X, 'target': y})\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.reshape(-1, 1), y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit a polynomial regression model\n",
    "\n",
    "poly_features = PolynomialFeatures(degree=3)\n",
    "\n",
    "X_train_poly = poly_features.fit_transform(X_train)\n",
    "\n",
    "X_test_poly = poly_features.transform(X_test)\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "model.fit(X_train_poly, y_train)\n",
    "\n",
    "# Make predictions\n",
    "\n",
    "y_pred = model.predict(X_test_poly)\n",
    "\n",
    "# Plot the data points and regression curve\n",
    "\n",
    "plt.scatter(X_test, y_test)\n",
    "plt.plot(X_test, y_pred, color='red')\n",
    "plt.title('Synthetic Dataset (Polynomial Regression)')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18. Write a Python script that performs multiple linear regression on a synthetic dataset with 5 features. Print the R-squared score and model coefficients.\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "X = np.random.rand(1000, 5)\n",
    "\n",
    "y = 2 * X[:, 0] + 3 * X[:, 1] - 4 * X[:, 2] + 5 * X[:, 3] + 6 * X[:, 4] + np.random.randn(1000)\n",
    "\n",
    "df = pd.DataFrame(np.c_[X, y], columns=['feature1', 'feature2', 'feature3', 'feature4', 'feature5', 'target'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "coefficients = model.coef_\n",
    "\n",
    "intercept = model.intercept_\n",
    "\n",
    "r2 = r2_score(y_test, model.predict(X_test))\n",
    "\n",
    "print(\"Coefficients:\", coefficients)\n",
    "print(\"Intercept:\", intercept)\n",
    "print(\"R-squared Score:\", r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 19. Write a Python script that generates synthetic data for linear regression, fits a model, and visualizes the data points along with the regression line.\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "X = np.linspace(0, 10, 100)\n",
    "\n",
    "y = 2 * X + np.random.randn(100)\n",
    "\n",
    "df = pd.DataFrame({'feature': X, 'target': y})\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.reshape(-1, 1), y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test.reshape(-1, 1))\n",
    "\n",
    "plt.scatter(X_test, y_test)\n",
    "plt.plot(X_test, y_pred, color='red')\n",
    "plt.title('Synthetic Dataset (Linear Regression)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20. Create a synthetic dataset with 3 features and perform multiple linear regression. Print the model's Rsquared score and coefficients.\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "X = np.random.rand(1000, 3)\n",
    "\n",
    "y = 2 * X[:, 0] + 3 * X[:, 1] - 4 * X[:, 2] + np.random.randn(1000)\n",
    "\n",
    "df = pd.DataFrame(np.c_[X, y], columns=['feature1', 'feature2', 'feature3', 'target'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "coefficients = model.coef_\n",
    "\n",
    "intercept = model.intercept_\n",
    "\n",
    "r2 = r2_score(y_test, model.predict(X_test))\n",
    "\n",
    "print(\"Coefficients:\", coefficients)\n",
    "print(\"Intercept:\", intercept)\n",
    "print(\"R-squared Score:\", r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 21. Write a Python script that demonstrates how to serialize and deserialize machine learning models using joblib instead of pickling.\n",
    "\n",
    "from joblib import dump, load\n",
    "\n",
    "# Serialize the model\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "dump(model, 'linear_regression_model.joblib')\n",
    "\n",
    "# Deserialize the model\n",
    "\n",
    "loaded_model = load('linear_regression_model.joblib')\n",
    "\n",
    "predictions = loaded_model.predict(X_test)\n",
    "\n",
    "print(\"Coefficients:\", loaded_model.coef_)\n",
    "\n",
    "print(\"Intercept:\", loaded_model.intercept_)\n",
    "\n",
    "print(\"R-squared Score:\", r2_score(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 22. Write a Python script to perform linear regression with categorical features using one-hot encoding. Use the Seaborn 'tips' dataset.\n",
    "\n",
    "tips = sns.load_dataset('tips')\n",
    "\n",
    "# One-hot encodin\n",
    "\n",
    "encoded_tips = pd.get_dummies(tips, columns=['sex','smoker', 'day', 'time'])\n",
    "\n",
    "X = encoded_tips.drop('tip', axis=1)\n",
    "\n",
    "y = encoded_tips['tip']\n",
    "\n",
    "# Perform linear regression\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "model.fit(X, y)\n",
    "\n",
    "predictions = model.predict(X)\n",
    "\n",
    "print(\"Coefficients:\", model.coef_)\n",
    "\n",
    "print(\"R-squared Score:\", r2_score(y, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 23. Compare Ridge Regression with Linear Regression on a synthetic dataset and print the coefficients and Rsquared score.\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "np.random.seed(42)\n",
    "X = np.linspace(0, 10, 100)\n",
    "y = 2 * X + np.random.randn(100)\n",
    "\n",
    "df = pd.DataFrame({'feature': X, 'target': y})\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.reshape(-1, 1), y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Linear Regression\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "coefficients_linear = model.coef_\n",
    "\n",
    "intercept_linear = model.intercept_\n",
    "\n",
    "r2_linear = r2_score(y_test, model.predict(X_test))\n",
    "\n",
    "# Ridge Regression\n",
    "\n",
    "model = Ridge(alpha=1.0)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "coefficients_ridge = model.coef_\n",
    "intercept_ridge = model.intercept_\n",
    "\n",
    "r2_ridge = r2_score(y_test, model.predict(X_test))\n",
    "\n",
    "print(\"Linear Regression Coefficients:\", coefficients_linear)\n",
    "print(\"Linear Regression Intercept:\", intercept_linear)\n",
    "print(\"Linear Regression R-squared Score:\", r2_linear)\n",
    "print(\"Ridge Regresion Coefficients:\", coefficients_ridge)\n",
    "print(\"Ridge Regression Intercept:\", intercept_ridge)\n",
    "print(\"Ridge Regression R-squared Score:\", r2_ridge)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 24. Write a Python script that uses cross-validation to evaluate a Linear Regression model on a synthetic dataset.\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "X = np.linspace(0, 10, 100)\n",
    "\n",
    "y = 2 * X + np.random.randn(100)\n",
    "\n",
    "df = pd.DataFrame({'feature': X, 'target': y})\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.reshape(-1, 1), y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the polynomial degrees to evaluate\n",
    "\n",
    "degrees = [1, 2, 3, 4, 5]\n",
    "\n",
    "# Perform cross-validation\n",
    "\n",
    "r2_scores = []\n",
    "\n",
    "for degree in degrees:\n",
    "\n",
    "    poly_features = PolynomialFeatures(degree=degree)\n",
    "    X_train_poly = poly_features.fit_transform(X_train)\n",
    "    X_test_poly = poly_features.transform(X_test)\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_poly, y_train)\n",
    "    r2_scores.append(r2_score(y_test, model.predict(X_test_poly)))\n",
    "    print(f\"Degree {degree}: R-squared Score: {r2_scores[-1]}\")\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 25. Write a Python script that compares polynomial regression models of different degrees and prints the Rsquared score for each\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "X = np.linspace(0, 10, 100)\n",
    "\n",
    "y = 2 * X + np.random.randn(100)\n",
    "\n",
    "df = pd.DataFrame({'feature': X, 'target': y})\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.reshape(-1, 1), y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the polynomial degrees to evaluate\n",
    "\n",
    "degrees = [1, 2, 3, 4, 5]\n",
    "\n",
    "# Perform cross-validation\n",
    "\n",
    "r2_scores = []\n",
    "\n",
    "for degree in degrees:\n",
    "\n",
    "    poly_features = PolynomialFeatures(degree=degree)\n",
    "    X_train_poly = poly_features.fit_transform(X_train)\n",
    "    X_test_poly = poly_features.transform(X_test)\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_poly, y_train)\n",
    "    r2_scores.append(r2_score(y_test, model.predict(X_test_poly)))\n",
    "    print(f\"Degree {degree}: R-squared Score: {r2_scores[-1]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
